{"url_hash": "3d1d26937b35eda04f56c4985eb62cde", "first_paragraph": "", "has_infobox": false, "num_categories": 5, "num_images": 27, "num_infobox_rows": 0, "num_references": 181, "num_sections": 51, "page_length_chars": 110229, "scraped_at": "2026-01-07T00:24:05.185065", "title": "Machine learning", "url": "https://en.wikipedia.org/wiki/Machine_learning"}
{"url_hash": "bd36faa631da3638db3c4324329ee17b", "first_paragraph": "The following outline is provided as an overview of, and topical guide to, machine learning:", "has_infobox": false, "num_categories": 5, "num_images": 8, "num_infobox_rows": 0, "num_references": 3, "num_sections": 34, "page_length_chars": 29158, "scraped_at": "2026-01-07T00:24:07.546326", "title": "Outline of machine learning", "url": "https://en.wikipedia.org/wiki/Outline_of_machine_learning"}
{"url_hash": "070305d05602108c7fc79d4bcf9f2372", "first_paragraph": "80 Million Tiny Images is a dataset intended for training machine-learning systems constructed by Antonio Torralba, Rob Fergus, and William T. Freeman in a collaboration between MIT and New York University. It was published in 2008.", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 7, "num_sections": 5, "page_length_chars": 3830, "scraped_at": "2026-01-07T00:24:09.658862", "title": "80 Million Tiny Images", "url": "https://en.wikipedia.org/wiki/80_Million_Tiny_Images"}
{"url_hash": "c5a83d03bca6b6047c4747a46dfd9971", "first_paragraph": "\"A Logical Calculus of the Ideas Immanent in Nervous Activity\" is a 1943 article written by Warren McCulloch and Walter Pitts.[1] The paper, published in the journal The Bulletin of Mathematical Biophysics, proposed a mathematical model of the nervous system as a network of simple logical elements, later known as artificial neurons, or McCulloch-Pitts neurons. These neurons receive inputs, perform a weighted sum, and fire an output signal based on a threshold function. By connecting these units ", "has_infobox": false, "num_categories": 4, "num_images": 48, "num_infobox_rows": 0, "num_references": 18, "num_sections": 9, "page_length_chars": 14357, "scraped_at": "2026-01-07T00:24:11.671076", "title": "A Logical Calculus of the Ideas Immanent in Nervous Activity", "url": "https://en.wikipedia.org/wiki/A_Logical_Calculus_of_the_Ideas_Immanent_in_Nervous_Activity"}
{"url_hash": "91e08603f97ebc85ed32d2af2ab95016", "first_paragraph": "XLA (Accelerated Linear Algebra) is an open-source compiler for machine learning developed by the OpenXLA project.[1] XLA is designed to improve the performance of machine learning models by optimizing the computation graphs at a lower level, making it particularly useful for large-scale computations and high-performance machine learning models. Key features of XLA include:[2]", "has_infobox": true, "num_categories": 5, "num_images": 1, "num_infobox_rows": 8, "num_references": 11, "num_sections": 8, "page_length_chars": 5455, "scraped_at": "2026-01-07T00:24:13.283875", "title": "Accelerated Linear Algebra", "url": "https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra"}
{"url_hash": "b009dcc2e3bee537f40cfe9086268f46", "first_paragraph": "Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with the creation and modification of a software agent's knowledge about the effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in a logic-based action description language and used as input for automated planners.", "has_infobox": false, "num_categories": 3, "num_images": 11, "num_infobox_rows": 0, "num_references": 15, "num_sections": 6, "page_length_chars": 8029, "scraped_at": "2026-01-07T00:24:14.901043", "title": "Action model learning", "url": "https://en.wikipedia.org/wiki/Action_model_learning"}
{"url_hash": "bab4ca0aa13e7521045ce95189f09d5f", "first_paragraph": "Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary. [1][2][3] In statistics literature, it is sometimes also called optimal experimental design.[4] The information source is also called teacher", "has_infobox": false, "num_categories": 1, "num_images": 3, "num_infobox_rows": 0, "num_references": 17, "num_sections": 7, "page_length_chars": 14990, "scraped_at": "2026-01-07T00:24:16.960753", "title": "Active learning (machine learning)", "url": "https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"}
{"url_hash": "9cd3a5fff3d71a70b9cf0ff530a73296", "first_paragraph": "Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks.[1] A survey from May 2020 revealed practitioners' common feeling for better protection of machine learning systems in industrial applications.[2]", "has_infobox": false, "num_categories": 3, "num_images": 71, "num_infobox_rows": 0, "num_references": 119, "num_sections": 27, "page_length_chars": 58467, "scraped_at": "2026-01-07T00:24:19.533654", "title": "Adversarial machine learning", "url": "https://en.wikipedia.org/wiki/Adversarial_machine_learning"}
{"url_hash": "ffd43c447f0b23596eda8e2aba337138", "first_paragraph": "An AI data center (or artificial intelligence data center) is a specialized data center facility designed for the computationally intensive tasks of training and running inference for artificial intelligence (AI) and machine learning models. Unlike general-purpose data centers, they are optimized for the parallel processing demands of AI workloads, typically utilizing hardware such as AI accelerators (e.g., GPUs, TPUs) and high-speed interconnects.", "has_infobox": false, "num_categories": 13, "num_images": 2, "num_infobox_rows": 0, "num_references": 99, "num_sections": 13, "page_length_chars": 37821, "scraped_at": "2026-01-07T00:24:22.107887", "title": "AI datacenter", "url": "https://en.wikipedia.org/wiki/AI_datacenter"}
{"url_hash": "c6c98937dee9372e675bf652e6ff0107", "first_paragraph": "AIOps (Artificial Intelligence for IT Operations) refers to the use of artificial intelligence, machine learning, and big data analytics to automate and enhance data center management. It helps organizations manage complex IT environments by detecting, diagnosing, and resolving issues more efficiently than traditional methods.[1][2]", "has_infobox": false, "num_categories": 3, "num_images": 1, "num_infobox_rows": 0, "num_references": 27, "num_sections": 8, "page_length_chars": 8918, "scraped_at": "2026-01-07T00:24:24.387774", "title": "AIOps", "url": "https://en.wikipedia.org/wiki/AIOps"}
{"url_hash": "a78b00a6615e89b274251d9c44549472", "first_paragraph": "AIXI /ˈaɪksi/ is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and several results regarding AIXI are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]", "has_infobox": false, "num_categories": 3, "num_images": 69, "num_infobox_rows": 0, "num_references": 7, "num_sections": 7, "page_length_chars": 9197, "scraped_at": "2026-01-07T00:24:26.688070", "title": "AIXI", "url": "https://en.wikipedia.org/wiki/AIXI"}
{"url_hash": "da0c7ff2adde4593e71fe911ab86f00c", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 27, "num_infobox_rows": 0, "num_references": 12, "num_sections": 19, "page_length_chars": 11508, "scraped_at": "2026-01-07T00:24:28.752874", "title": "Algorithm selection", "url": "https://en.wikipedia.org/wiki/Algorithm_selection"}
{"url_hash": "03a3ae18f9e582251dac4193d8945a43", "first_paragraph": "", "has_infobox": false, "num_categories": 6, "num_images": 6, "num_infobox_rows": 0, "num_references": 219, "num_sections": 37, "page_length_chars": 111482, "scraped_at": "2026-01-07T00:24:30.358767", "title": "Algorithmic bias", "url": "https://en.wikipedia.org/wiki/Algorithmic_bias"}
{"url_hash": "ca869ab4e78d0c745cdbebb4bb6869c0", "first_paragraph": "Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966). The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results.", "has_infobox": false, "num_categories": 2, "num_images": 56, "num_infobox_rows": 0, "num_references": 1, "num_sections": 10, "page_length_chars": 12852, "scraped_at": "2026-01-07T00:24:33.179705", "title": "Algorithmic inference", "url": "https://en.wikipedia.org/wiki/Algorithmic_inference"}
{"url_hash": "3bba5c970d8f41b6bd4492e1d32f9118", "first_paragraph": "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]", "has_infobox": false, "num_categories": 5, "num_images": 2, "num_infobox_rows": 0, "num_references": 58, "num_sections": 28, "page_length_chars": 31130, "scraped_at": "2026-01-07T00:24:35.513086", "title": "Anomaly detection", "url": "https://en.wikipedia.org/wiki/Anomaly_detection"}
{"url_hash": "ffec071d59d66f8f0f0a9afb81078531", "first_paragraph": "Aporia is a machine learning observability platform based in Tel Aviv, Israel. The company has a US office located in San Jose, California.[1][2][3][4]", "has_infobox": true, "num_categories": 4, "num_images": 1, "num_infobox_rows": 7, "num_references": 19, "num_sections": 2, "page_length_chars": 4136, "scraped_at": "2026-01-07T00:24:37.875263", "title": "Aporia (company)", "url": "https://en.wikipedia.org/wiki/Aporia_(company)"}
{"url_hash": "e5381080b16a98cb6b5c3d406b975e3a", "first_paragraph": "In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert.[1][2] It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.[2]", "has_infobox": false, "num_categories": 1, "num_images": 0, "num_infobox_rows": 0, "num_references": 14, "num_sections": 7, "page_length_chars": 8879, "scraped_at": "2026-01-07T00:24:39.480248", "title": "Apprenticeship learning", "url": "https://en.wikipedia.org/wiki/Apprenticeship_learning"}
{"url_hash": "4321b7a23153af810ae41163f341ec43", "first_paragraph": "Artificial intelligence can be used to automate aspects of the job recruitment process. Advances in artificial intelligence, such as the advent of machine learning and the growth of big data, enable AI to be utilized to recruit, screen, and predict the success of applicants.[1][2] Proponents of artificial intelligence in hiring claim it reduces bias, assists with finding qualified candidates, and frees up human resource workers' time for other tasks, while opponents worry that AI perpetuates ine", "has_infobox": false, "num_categories": 4, "num_images": 2, "num_infobox_rows": 0, "num_references": 42, "num_sections": 10, "page_length_chars": 26091, "scraped_at": "2026-01-07T00:24:41.807954", "title": "Artificial intelligence in hiring", "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_hiring"}
{"url_hash": "809495acc0684925826f5a148e75534b", "first_paragraph": "Astrostatistics is a discipline which spans astrophysics, statistical analysis and data mining.[1] It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory. Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference. The field is c", "has_infobox": false, "num_categories": 6, "num_images": 5, "num_infobox_rows": 0, "num_references": 1, "num_sections": 1, "page_length_chars": 1211, "scraped_at": "2026-01-07T00:24:43.505165", "title": "Astrostatistics", "url": "https://en.wikipedia.org/wiki/Astrostatistics"}
{"url_hash": "f75cf54ebd91127409e0f853b8ec104a", "first_paragraph": "In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size.", "has_infobox": false, "num_categories": 1, "num_images": 63, "num_infobox_rows": 0, "num_references": 57, "num_sections": 20, "page_length_chars": 25418, "scraped_at": "2026-01-07T00:24:45.607366", "title": "Attention (machine learning)", "url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)"}
{"url_hash": "4d90c8eb715f91beced22504277926e9", "first_paragraph": "Audio inpainting (also known as audio interpolation) is an audio restoration task which deals with the reconstruction of missing or corrupted portions of a digital audio signal.[1] Inpainting techniques are employed when parts of the audio have been lost due to various factors such as transmission errors, data corruption or errors during recording.[2]", "has_infobox": false, "num_categories": 3, "num_images": 17, "num_infobox_rows": 0, "num_references": 20, "num_sections": 7, "page_length_chars": 15239, "scraped_at": "2026-01-07T00:24:47.901674", "title": "Audio inpainting", "url": "https://en.wikipedia.org/wiki/Audio_inpainting"}
{"url_hash": "0dc856796d1c733810ab35120d21b090", "first_paragraph": "Automated decision-making (ADM) is the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM may involve large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, ", "has_infobox": false, "num_categories": 4, "num_images": 0, "num_infobox_rows": 0, "num_references": 46, "num_sections": 23, "page_length_chars": 30371, "scraped_at": "2026-01-07T00:24:49.976000", "title": "Automated decision-making", "url": "https://en.wikipedia.org/wiki/Automated_decision-making"}
{"url_hash": "cf546d54bcba69734c4ae557eb27bc71", "first_paragraph": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML.[1]", "has_infobox": false, "num_categories": 3, "num_images": 1, "num_infobox_rows": 0, "num_references": 9, "num_sections": 6, "page_length_chars": 8021, "scraped_at": "2026-01-07T00:24:51.594872", "title": "Automated machine learning", "url": "https://en.wikipedia.org/wiki/Automated_machine_learning"}
{"url_hash": "de936b410098229b886b5c86b785f9b1", "first_paragraph": "Automation in construction is the combination of methods, processes, and systems that allow for greater machine autonomy in construction activities. Construction automation may have multiple goals, including but not limited to, reducing jobsite injuries, decreasing activity completion times, and assisting with quality control and quality assurance.[1] Some systems may be fielded as a direct response to increasing skilled labor shortages in some countries.[2] Opponents claim that increased automa", "has_infobox": false, "num_categories": 6, "num_images": 0, "num_infobox_rows": 0, "num_references": 8, "num_sections": 4, "page_length_chars": 4285, "scraped_at": "2026-01-07T00:24:53.600880", "title": "Automation in construction", "url": "https://en.wikipedia.org/wiki/Automation_in_construction"}
{"url_hash": "eefe54064b77426814699be3906dd555", "first_paragraph": "The bag-of-words (BoW) model is a model of text which uses an unordered collection (a \"bag\") of words. It is used in natural language processing and information retrieval (IR). It disregards word order (and thus most of syntax or grammar) but captures multiplicity.", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 5, "num_sections": 8, "page_length_chars": 8161, "scraped_at": "2026-01-07T00:24:56.009039", "title": "Bag-of-words model", "url": "https://en.wikipedia.org/wiki/Bag-of-words_model"}
{"url_hash": "8acd5a7d605f6abbe8ac27a7fa41e465", "first_paragraph": "In computer science, a ball tree, balltree[1] or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. A ball tree partitions data points into a nested set of balls. The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search.", "has_infobox": false, "num_categories": 2, "num_images": 5, "num_infobox_rows": 0, "num_references": 4, "num_sections": 8, "page_length_chars": 8972, "scraped_at": "2026-01-07T00:24:57.756050", "title": "Ball tree", "url": "https://en.wikipedia.org/wiki/Ball_tree"}
{"url_hash": "080186b7566934adb37dc06a20457d78", "first_paragraph": "In probability and statistics, the base rate (also known as prior probabilities) is the class of probabilities unconditional on \"featural evidence\" (likelihoods).", "has_infobox": false, "num_categories": 4, "num_images": 0, "num_infobox_rows": 0, "num_references": 6, "num_sections": 3, "page_length_chars": 4911, "scraped_at": "2026-01-07T00:24:59.633001", "title": "Base rate", "url": "https://en.wikipedia.org/wiki/Base_rate"}
{"url_hash": "43e57cb83aaf7bf4b659352ca7e09fc0", "first_paragraph": "Bayesian interpretation of kernel regularization examines how kernel methods in machine learning can be understood through the lens of Bayesian statistics, a framework that uses probability to model uncertainty. Kernel methods are founded on the concept of similarity between inputs within a structured space. While techniques like support vector machines (SVMs) and their regularization (a technique to make a model more generalizable and transferable) were not originally formulated using Bayesian ", "has_infobox": false, "num_categories": 2, "num_images": 78, "num_infobox_rows": 0, "num_references": 13, "num_sections": 12, "page_length_chars": 11153, "scraped_at": "2026-01-07T00:25:02.235415", "title": "Bayesian interpretation of kernel regularization", "url": "https://en.wikipedia.org/wiki/Bayesian_interpretation_of_kernel_regularization"}
{"url_hash": "e91487c01e3caa70e3b0e44cfb0eeafe", "first_paragraph": "Bayesian learning mechanisms are probabilistic causal models[1] used in computer science to research the fundamental underpinnings of machine learning, and in cognitive neuroscience, to model conceptual development.[2][3]", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 6, "num_sections": 2, "page_length_chars": 2424, "scraped_at": "2026-01-07T00:25:04.280393", "title": "Bayesian learning mechanisms", "url": "https://en.wikipedia.org/wiki/Bayesian_learning_mechanisms"}
{"url_hash": "1ac354a0f984a882dcec284e272fb038", "first_paragraph": "Bayesian optimization is a sequential design strategy for global optimization of black-box functions,[1][2][3] that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions. With the rise of artificial intelligence innovation in the 21st century, Bayesian optimization algorithms have found prominent use in machine learning problems for optimizing hyperparameter values.[4][5]", "has_infobox": false, "num_categories": 4, "num_images": 10, "num_infobox_rows": 0, "num_references": 45, "num_sections": 8, "page_length_chars": 16724, "scraped_at": "2026-01-07T00:25:06.205734", "title": "Bayesian optimization", "url": "https://en.wikipedia.org/wiki/Bayesian_optimization"}
{"url_hash": "f5a071c5a1f8ae701f7302c7e4e62c07", "first_paragraph": "In stochastic game theory, Bayesian regret is the expected difference (\"regret\") between the utility of a given strategy and the utility of the best possible strategy in hindsight—i.e., the strategy that would have maximized expected payoff if the true underlying model or distribution were known. This notion of regret measures how much is lost, on average, due to uncertainty or imperfect information.", "has_infobox": false, "num_categories": 6, "num_images": 2, "num_infobox_rows": 0, "num_references": 7, "num_sections": 3, "page_length_chars": 2911, "scraped_at": "2026-01-07T00:25:08.477051", "title": "Bayesian regret", "url": "https://en.wikipedia.org/wiki/Bayesian_regret"}
{"url_hash": "fcb2ec03f97e5de88398b3f7c0fff81e", "first_paragraph": "Bayesian structural time series (BSTS) model is a statistical technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other applications. The model is designed to work with time series data.", "has_infobox": false, "num_categories": 3, "num_images": 1, "num_infobox_rows": 0, "num_references": 4, "num_sections": 4, "page_length_chars": 3516, "scraped_at": "2026-01-07T00:25:10.004164", "title": "Bayesian structural time series", "url": "https://en.wikipedia.org/wiki/Bayesian_structural_time_series"}
{"url_hash": "2eb21152461822865df7c634f4bed332", "first_paragraph": "In statistics and machine learning, the bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as the number of tunable parameters in a model increases, it becomes more flexible, and can better fit a training data set. That is, the model has lower error or lower bias. However, for more flexible models, there will tend to be gr", "has_infobox": false, "num_categories": 4, "num_images": 74, "num_infobox_rows": 0, "num_references": 25, "num_sections": 14, "page_length_chars": 20595, "scraped_at": "2026-01-07T00:25:12.816385", "title": "Bias–variance tradeoff", "url": "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"}
{"url_hash": "b2596421ac080ade5a2ad458daa52d94", "first_paragraph": "Binary classification is the task of putting things into one of two categories (each called a class). As such, it is the simplest form of the general task of classification into any number of classes. Typical binary classification problems include:", "has_infobox": false, "num_categories": 2, "num_images": 8, "num_infobox_rows": 0, "num_references": 3, "num_sections": 9, "page_length_chars": 11881, "scraped_at": "2026-01-07T00:25:14.769927", "title": "Binary classification", "url": "https://en.wikipedia.org/wiki/Binary_classification"}
{"url_hash": "76719df061ab269c1422e515112780f4", "first_paragraph": "", "has_infobox": true, "num_categories": 4, "num_images": 1, "num_infobox_rows": 11, "num_references": 25, "num_sections": 4, "page_length_chars": 6525, "scraped_at": "2026-01-07T00:25:17.355994", "title": "Bioserenity", "url": "https://en.wikipedia.org/wiki/Bioserenity"}
{"url_hash": "a5dc174e77abb852957032f6cae71540", "first_paragraph": "The Bradley–Terry model is a probability model for the outcome of pairwise comparisons between items, teams, or objects. Given a pair of items i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as", "has_infobox": false, "num_categories": 4, "num_images": 50, "num_infobox_rows": 0, "num_references": 17, "num_sections": 9, "page_length_chars": 11394, "scraped_at": "2026-01-07T00:25:20.137250", "title": "Bradley–Terry model", "url": "https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model"}
{"url_hash": "647e167c8d59592b28df8d18fd805cd2", "first_paragraph": "Category utility is a measure of \"category goodness\" defined in Gluck & Corter (1985)[1] and Corter & Gluck (1992).[2] It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as \"cue validity\"[3][4] and \"collocation index\".[5] It provides a normative information-theoreti", "has_infobox": false, "num_categories": 2, "num_images": 85, "num_infobox_rows": 0, "num_references": 19, "num_sections": 11, "page_length_chars": 17229, "scraped_at": "2026-01-07T00:25:22.504868", "title": "Category utility", "url": "https://en.wikipedia.org/wiki/Category_utility"}
{"url_hash": "a9cae7a6726c35d951eef878ec1f5eaa", "first_paragraph": "The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative. Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning. This includes CIML research-, education, and application-oriented resources residing at the portal and others that are linked from the CIML site.", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 3, "num_sections": 5, "page_length_chars": 3756, "scraped_at": "2026-01-07T00:25:24.185378", "title": "CIML community portal", "url": "https://en.wikipedia.org/wiki/CIML_community_portal"}
{"url_hash": "3e6676f21195d03f3be95abb795ce78f", "first_paragraph": "", "has_infobox": true, "num_categories": 7, "num_images": 9, "num_infobox_rows": 11, "num_references": 67, "num_sections": 12, "page_length_chars": 22816, "scraped_at": "2026-01-07T00:25:26.596713", "title": "Claude (language model)", "url": "https://en.wikipedia.org/wiki/Claude_(language_model)"}
{"url_hash": "08c4d7b87a20c415aba98c441464dd01", "first_paragraph": "Cognitive robotics or cognitive technology is a subfield of robotics concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition, consisting of robotic process automation, artificial intelligence, machine learning, deep learn", "has_infobox": false, "num_categories": 3, "num_images": 4, "num_infobox_rows": 0, "num_references": 3, "num_sections": 12, "page_length_chars": 8253, "scraped_at": "2026-01-07T00:25:28.754708", "title": "Cognitive robotics", "url": "https://en.wikipedia.org/wiki/Cognitive_robotics"}
{"url_hash": "9016fbf222e51c1be313c2f46a97a602", "first_paragraph": "In predictive analytics, data science, machine learning and related fields, concept drift or drift is an evolution of data that invalidates the data model. It happens when the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes. Drift detection and drift adaptation are of paramount importance in the fields that involve dynamically changing data an", "has_infobox": false, "num_categories": 3, "num_images": 1, "num_infobox_rows": 0, "num_references": 25, "num_sections": 18, "page_length_chars": 32328, "scraped_at": "2026-01-07T00:25:30.437515", "title": "Concept drift", "url": "https://en.wikipedia.org/wiki/Concept_drift"}
{"url_hash": "7fd5c822b366a6c9733e51e5ba452609", "first_paragraph": "Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. The kind of graph used depends on the application. For exa", "has_infobox": false, "num_categories": 2, "num_images": 49, "num_infobox_rows": 0, "num_references": 14, "num_sections": 10, "page_length_chars": 12811, "scraped_at": "2026-01-07T00:25:32.562076", "title": "Conditional random field", "url": "https://en.wikipedia.org/wiki/Conditional_random_field"}
{"url_hash": "a481fcff67130878e25f2878d487a109", "first_paragraph": "In machine learning, a confusion matrix, also known as error matrix,[1] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one. In unsupervised learning it is usually called a matching matrix. The term is used specifically in the problem of statistical classification.", "has_infobox": false, "num_categories": 2, "num_images": 5, "num_infobox_rows": 0, "num_references": 28, "num_sections": 6, "page_length_chars": 18374, "scraped_at": "2026-01-07T00:25:34.714037", "title": "Confusion matrix", "url": "https://en.wikipedia.org/wiki/Confusion_matrix"}
{"url_hash": "b6dfdd703660ce5c2279f794ead9c290", "first_paragraph": "Contrastive Language-Image Pre-training (CLIP) is a technique for training a pair of neural network models, one for image understanding and one for text understanding, using a contrastive objective.[1] This method has enabled broad applications across multiple domains, including cross-modal retrieval,[2] text-to-image generation,[3] and aesthetic ranking.[4]", "has_infobox": true, "num_categories": 4, "num_images": 15, "num_infobox_rows": 8, "num_references": 38, "num_sections": 16, "page_length_chars": 20357, "scraped_at": "2026-01-07T00:25:37.085927", "title": "Contrastive Language-Image Pre-training", "url": "https://en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training"}
{"url_hash": "a238ab7eebdd953f967ee7b5aa95e05f", "first_paragraph": "Cost-sensitive machine learning[1][2] is an approach within machine learning that considers varying costs associated with different types of errors. This method diverges from traditional approaches by introducing a cost matrix, explicitly specifying the penalties or benefits for each type of prediction error. The inherent difficulty which cost-sensitive machine learning tackles is that minimizing different kinds of classification errors is a multi-objective optimization problem.", "has_infobox": false, "num_categories": 2, "num_images": 9, "num_infobox_rows": 0, "num_references": 2, "num_sections": 8, "page_length_chars": 3869, "scraped_at": "2026-01-07T00:25:39.713428", "title": "Cost-sensitive machine learning", "url": "https://en.wikipedia.org/wiki/Cost-sensitive_machine_learning"}
{"url_hash": "7e20cb5b069b770fb26e55726b812e84", "first_paragraph": "Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.", "has_infobox": false, "num_categories": 1, "num_images": 0, "num_infobox_rows": 0, "num_references": 3, "num_sections": 12, "page_length_chars": 8078, "scraped_at": "2026-01-07T00:25:41.604633", "title": "Coupled pattern learner", "url": "https://en.wikipedia.org/wiki/Coupled_pattern_learner"}
{"url_hash": "7cd6fbe742b9bbb68810a153103243cc", "first_paragraph": "Croissant is a metadata format design to support sharing of datasets for machine learning applications. It is a platform-agnostic schema used to standardize metadata in data repositories like Hugging Face, kaggle, Dataverse and OpenML.[1][2]", "has_infobox": false, "num_categories": 4, "num_images": 0, "num_infobox_rows": 0, "num_references": 11, "num_sections": 4, "page_length_chars": 4983, "scraped_at": "2026-01-07T00:25:43.736743", "title": "Croissant (metadata format)", "url": "https://en.wikipedia.org/wiki/Croissant_(metadata_format)"}
{"url_hash": "39446754da017f45c16a2975dc3c8b35", "first_paragraph": "The cross-entropy (CE) method is a Monte Carlo method for importance sampling and optimization. It is applicable to both combinatorial and continuous problems, with either a static or noisy objective.", "has_infobox": false, "num_categories": 4, "num_images": 34, "num_infobox_rows": 0, "num_references": 1, "num_sections": 9, "page_length_chars": 5028, "scraped_at": "2026-01-07T00:25:46.039426", "title": "Cross-entropy method", "url": "https://en.wikipedia.org/wiki/Cross-entropy_method"}
{"url_hash": "2f02ed4ea32a43eebb80fe23486adaa6", "first_paragraph": "Cross-validation,[2][3][4] sometimes called rotation estimation[5][6][7] or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how a", "has_infobox": false, "num_categories": 3, "num_images": 37, "num_infobox_rows": 0, "num_references": 41, "num_sections": 19, "page_length_chars": 37592, "scraped_at": "2026-01-07T00:25:48.321105", "title": "Cross-validation (statistics)", "url": "https://en.wikipedia.org/wiki/Cross-validation_(statistics)"}
{"url_hash": "3d3d92df48643e720e989c36352b8b1d", "first_paragraph": "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic programming.[1][2] The curse generally refers to issues that arise when the number of datapoints is small (in a suitably defined sense) relative to the intrinsic dimension", "has_infobox": false, "num_categories": 4, "num_images": 35, "num_infobox_rows": 0, "num_references": 29, "num_sections": 12, "page_length_chars": 26171, "scraped_at": "2026-01-07T00:25:51.036544", "title": "Curse of dimensionality", "url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality"}
{"url_hash": "169dd94b3160c71c46b84608a1fe87b5", "first_paragraph": "Data augmentation is a statistical technique which allows maximum likelihood estimation from incomplete data.[1][2] Data augmentation has important applications in Bayesian analysis,[3] and the technique is widely used in machine learning to reduce overfitting when training machine learning models,[4] achieved by training models on several slightly-modified copies of existing data.", "has_infobox": false, "num_categories": 1, "num_images": 11, "num_infobox_rows": 0, "num_references": 18, "num_sections": 10, "page_length_chars": 13530, "scraped_at": "2026-01-07T00:25:52.652116", "title": "Data augmentation", "url": "https://en.wikipedia.org/wiki/Data_augmentation"}
{"url_hash": "0a0d4272512352e9b72cef04794e292c", "first_paragraph": "Data exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems.[1] These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.", "has_infobox": false, "num_categories": 4, "num_images": 0, "num_infobox_rows": 0, "num_references": 6, "num_sections": 4, "page_length_chars": 4236, "scraped_at": "2026-01-07T00:25:55.139817", "title": "Data exploration", "url": "https://en.wikipedia.org/wiki/Data_exploration"}
{"url_hash": "ebbfeefc647546bd47f54b0f3ed129c0", "first_paragraph": "Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed,[1] and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues. Preprocessing is the process by which unstructured data is transformed into intelligible representations suitable for machine-learning models. This phase of model deals with ", "has_infobox": false, "num_categories": 2, "num_images": 4, "num_infobox_rows": 0, "num_references": 13, "num_sections": 5, "page_length_chars": 13162, "scraped_at": "2026-01-07T00:25:57.154017", "title": "Data preprocessing", "url": "https://en.wikipedia.org/wiki/Data_preprocessing"}
{"url_hash": "02eb3d2003ffc4fc9c47373bd5df8495", "first_paragraph": "Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies.[2][3] The field is closely related to astrostatistics.", "has_infobox": false, "num_categories": 6, "num_images": 9, "num_infobox_rows": 0, "num_references": 46, "num_sections": 11, "page_length_chars": 19989, "scraped_at": "2026-01-07T00:25:59.439907", "title": "Data-driven astronomy", "url": "https://en.wikipedia.org/wiki/Data-driven_astronomy"}
{"url_hash": "2b2a196da0067806f93add2592d4db67", "first_paragraph": "Data-driven models are a class of computational models that primarily rely on historical data collected throughout a system's or process' lifetime to establish relationships between input, internal, and output variables. Commonly found in numerous articles and publications, data-driven models have evolved from earlier statistical models, overcoming limitations posed by strict assumptions about probability distributions. These models have gained prominence across various fields, particularly in t", "has_infobox": false, "num_categories": 4, "num_images": 0, "num_infobox_rows": 0, "num_references": 14, "num_sections": 3, "page_length_chars": 5679, "scraped_at": "2026-01-07T00:26:01.231005", "title": "Data-driven model", "url": "https://en.wikipedia.org/wiki/Data-driven_model"}
{"url_hash": "2a89bb7a698dda5421f22ed5f256a620", "first_paragraph": "Decision lists are a representation for Boolean functions which can be easily learnable from examples.[1] Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 2, "num_sections": 3, "page_length_chars": 1460, "scraped_at": "2026-01-07T00:26:03.356607", "title": "Decision list", "url": "https://en.wikipedia.org/wiki/Decision_list"}
{"url_hash": "bc27ac573da009f6aec9b0a734d8e805", "first_paragraph": "Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.", "has_infobox": false, "num_categories": 2, "num_images": 15, "num_infobox_rows": 0, "num_references": 1, "num_sections": 11, "page_length_chars": 5873, "scraped_at": "2026-01-07T00:26:05.289072", "title": "Decision tree pruning", "url": "https://en.wikipedia.org/wiki/Decision_tree_pruning"}
{"url_hash": "37cbaaa934b8e26e8993e3da0f398511", "first_paragraph": "Deep Tomographic Reconstruction is a set of methods for using deep learning methods to perform tomographic reconstruction of medical and industrial images. It uses artificial intelligence and machine learning, especially deep artificial neural networks or deep learning, to overcome challenges such as measurement noise, data sparsity, image artifacts, and computational inefficiency. This approach has been applied across various imaging modalities, including CT, MRI, PET, SPECT, ultrasound, and op", "has_infobox": false, "num_categories": 1, "num_images": 0, "num_infobox_rows": 0, "num_references": 27, "num_sections": 8, "page_length_chars": 11666, "scraped_at": "2026-01-07T00:26:07.175568", "title": "Deep tomographic reconstruction", "url": "https://en.wikipedia.org/wiki/Deep_tomographic_reconstruction"}
{"url_hash": "1bad5eeeb274d212ac93d951ec736f53", "first_paragraph": "Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approa", "has_infobox": false, "num_categories": 3, "num_images": 3, "num_infobox_rows": 0, "num_references": 5, "num_sections": 15, "page_length_chars": 19699, "scraped_at": "2026-01-07T00:26:09.145842", "title": "Developmental robotics", "url": "https://en.wikipedia.org/wiki/Developmental_robotics"}
{"url_hash": "7ef0cb3032b6b735e9cf8e63fa83c14d", "first_paragraph": "A discovery system is an artificial intelligence system that attempts to discover new scientific concepts or laws. The aim of discovery systems is to automate scientific data analysis and the scientific discovery process. Ideally, an artificial intelligence system should be able to search systematically through the space of all possible hypotheses and yield the hypothesis - or set of equally likely hypotheses - that best describes the complex patterns in data.[1][2]", "has_infobox": false, "num_categories": 5, "num_images": 2, "num_infobox_rows": 0, "num_references": 14, "num_sections": 4, "page_length_chars": 6215, "scraped_at": "2026-01-07T00:26:11.272720", "title": "Discovery system (artificial intelligence)", "url": "https://en.wikipedia.org/wiki/Discovery_system_(artificial_intelligence)"}
{"url_hash": "b3ed0119ed4679ccebd612aea836e67d", "first_paragraph": "Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, howe", "has_infobox": false, "num_categories": 5, "num_images": 0, "num_infobox_rows": 0, "num_references": 18, "num_sections": 9, "page_length_chars": 12038, "scraped_at": "2026-01-07T00:26:13.656633", "title": "Document classification", "url": "https://en.wikipedia.org/wiki/Document_classification"}
{"url_hash": "4fb106430dfb9fa77c065fe0bbdd9371", "first_paragraph": "Domain adaptation is a field associated with machine learning and transfer learning. It addresses the challenge of training a model on one data distribution (the source domain) and applying it to a related but different data distribution (the target domain).", "has_infobox": false, "num_categories": 1, "num_images": 25, "num_infobox_rows": 0, "num_references": 18, "num_sections": 11, "page_length_chars": 10903, "scraped_at": "2026-01-07T00:26:16.410734", "title": "Domain adaptation", "url": "https://en.wikipedia.org/wiki/Domain_adaptation"}
{"url_hash": "c4ca415ff33f4b50025e23375ec6725d", "first_paragraph": "Double descent in statistics and machine learning is the phenomenon where a model's error rate on the test set initially decreases with the number of parameters, then peaks, then decreases again.[2] This phenomenon has been considered surprising, as it contradicts assumptions about overfitting in classical machine learning.[3]", "has_infobox": false, "num_categories": 4, "num_images": 8, "num_infobox_rows": 0, "num_references": 16, "num_sections": 6, "page_length_chars": 8688, "scraped_at": "2026-01-07T00:26:18.691358", "title": "Double descent", "url": "https://en.wikipedia.org/wiki/Double_descent"}
{"url_hash": "422530aa12d63867c0ac3538cbb2692d", "first_paragraph": "In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system.[1] The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring mu", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 4, "num_sections": 1, "page_length_chars": 1821, "scraped_at": "2026-01-07T00:26:21.375005", "title": "Eager learning", "url": "https://en.wikipedia.org/wiki/Eager_learning"}
{"url_hash": "290ad8965a45232e8e37a359dabee558", "first_paragraph": "EfficientNet is a family of convolutional neural networks (CNNs) for computer vision published by researchers at Google AI in 2019.[1] Its key innovation is compound scaling, which uniformly scales all dimensions of depth, width, and resolution using a single parameter.", "has_infobox": true, "num_categories": 4, "num_images": 16, "num_infobox_rows": 7, "num_references": 6, "num_sections": 5, "page_length_chars": 3893, "scraped_at": "2026-01-07T00:26:23.075188", "title": "EfficientNet", "url": "https://en.wikipedia.org/wiki/EfficientNet"}
{"url_hash": "bc64d6df124386ee999e25a7c6faa348", "first_paragraph": "ELMo (embeddings from language model) is a word embedding method for representing a sequence of words as a corresponding sequence of vectors.[1] It was created by researchers at the Allen Institute for Artificial Intelligence,[2] and University of Washington and first released in February 2018. It is a bidirectional LSTM which takes character-level as inputs and produces word-level embeddings, trained on a corpus of about 30 million sentences and 1 billion words.", "has_infobox": false, "num_categories": 4, "num_images": 7, "num_infobox_rows": 0, "num_references": 4, "num_sections": 4, "page_length_chars": 6593, "scraped_at": "2026-01-07T00:26:24.678832", "title": "ELMo", "url": "https://en.wikipedia.org/wiki/ELMo"}
{"url_hash": "e413bdcc3d9dfdac4c63f232a544eb31", "first_paragraph": "In statistics, EM (expectation maximization) algorithm handles latent variables, while GMM is the Gaussian mixture model.", "has_infobox": false, "num_categories": 2, "num_images": 46, "num_infobox_rows": 0, "num_references": 3, "num_sections": 3, "page_length_chars": 3068, "scraped_at": "2026-01-07T00:26:26.665856", "title": "EM algorithm and GMM model", "url": "https://en.wikipedia.org/wiki/EM_algorithm_and_GMM_model"}
{"url_hash": "48528921a641605e7db564c6d8970982", "first_paragraph": "Embedding in machine learning refers to a representation learning technique that maps complex, high-dimensional data into a lower-dimensional vector space of numerical vectors.[1]", "has_infobox": false, "num_categories": 3, "num_images": 8, "num_infobox_rows": 0, "num_references": 5, "num_sections": 4, "page_length_chars": 2935, "scraped_at": "2026-01-07T00:26:28.319373", "title": "Embedding (machine learning)", "url": "https://en.wikipedia.org/wiki/Embedding_(machine_learning)"}
{"url_hash": "ab3915ad766ce618de17a832ef95fc69", "first_paragraph": "Empirical dynamic modeling (EDM) is a framework for analysis and prediction of nonlinear dynamical systems. Applications include population dynamics,[improper synthesis?][1][2][3][4][5][6] ecosystem service,[7] medicine,[8] neuroscience,[9][10][11] dynamical systems,[12][13][14] geophysics,[15][16][17] and human-computer interaction.[18] EDM was originally developed by Robert May and George Sugihara. It can be considered a methodology for data modeling, predictive analytics, dynamical system ana", "has_infobox": false, "num_categories": 5, "num_images": 59, "num_infobox_rows": 0, "num_references": 38, "num_sections": 12, "page_length_chars": 19075, "scraped_at": "2026-01-07T00:26:30.920762", "title": "Empirical dynamic modeling", "url": "https://en.wikipedia.org/wiki/Empirical_dynamic_modeling"}
{"url_hash": "1693fe72fb56ba1fca2e2c52c90242b3", "first_paragraph": "In statistical learning theory, the principle of empirical risk minimization defines a family of learning algorithms based on evaluating performance over a known and fixed dataset. The core idea is based on an application of the law of large numbers; more specifically, we cannot know exactly how well a predictive algorithm will work in practice (i.e. the \"true risk\") because we do not know the true distribution of the data, but we can instead estimate and optimize the performance of the algorith", "has_infobox": false, "num_categories": 1, "num_images": 55, "num_infobox_rows": 0, "num_references": 6, "num_sections": 9, "page_length_chars": 8253, "scraped_at": "2026-01-07T00:26:33.188737", "title": "Empirical risk minimization", "url": "https://en.wikipedia.org/wiki/Empirical_risk_minimization"}
{"url_hash": "fa15dccb5f11fb096179c8a7e39ea74a", "first_paragraph": "An energy-based model (EBM), also called Canonical Ensemble Learning (CEL) or Learning via Canonical Ensemble (LCE), is an application of canonical ensemble formulation from statistical physics for learning from data. The approach prominently appears in generative artificial intelligence.", "has_infobox": false, "num_categories": 4, "num_images": 29, "num_infobox_rows": 0, "num_references": 18, "num_sections": 12, "page_length_chars": 12279, "scraped_at": "2026-01-07T00:26:35.099347", "title": "Energy-based model", "url": "https://en.wikipedia.org/wiki/Energy-based_model"}
{"url_hash": "e904f7085e9ccff5b73aeda042d8deba", "first_paragraph": "Equalized odds,[1] also referred to as conditional procedure accuracy equality and disparate mistreatment, is a measure of fairness in machine learning. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal true positive rate and equal false positive rate,[2] satisfying the formula:", "has_infobox": false, "num_categories": 8, "num_images": 8, "num_infobox_rows": 0, "num_references": 3, "num_sections": 2, "page_length_chars": 1704, "scraped_at": "2026-01-07T00:26:37.223376", "title": "Equalized odds", "url": "https://en.wikipedia.org/wiki/Equalized_odds"}
{"url_hash": "6a6088a33b03393c43465a215c2dd7a0", "first_paragraph": "Evaluation of a binary classifier typically assigns a numerical value, or values, to a classifier that represent its accuracy. An example is error rate, which measures how frequently the classifier makes a mistake.", "has_infobox": false, "num_categories": 2, "num_images": 9, "num_infobox_rows": 0, "num_references": 20, "num_sections": 15, "page_length_chars": 22600, "scraped_at": "2026-01-07T00:26:38.971036", "title": "Evaluation of binary classifiers", "url": "https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers"}
{"url_hash": "153cd8d66724b620cfd5e16662df23de", "first_paragraph": "The term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries.", "has_infobox": false, "num_categories": 1, "num_images": 80, "num_infobox_rows": 0, "num_references": 0, "num_sections": 3, "page_length_chars": 4056, "scraped_at": "2026-01-07T00:26:41.046866", "title": "Evolvability (computer science)", "url": "https://en.wikipedia.org/wiki/Evolvability_(computer_science)"}
{"url_hash": "638d16a79baad31db636e792af3f5542", "first_paragraph": "Expectation propagation (EP) is a technique in Bayesian machine learning.[1]", "has_infobox": false, "num_categories": 3, "num_images": 12, "num_infobox_rows": 0, "num_references": 1, "num_sections": 3, "page_length_chars": 1555, "scraped_at": "2026-01-07T00:26:42.994288", "title": "Expectation propagation", "url": "https://en.wikipedia.org/wiki/Expectation_propagation"}
{"url_hash": "f9cb9e69f475d723c006ecab70e287a0", "first_paragraph": "Explanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory (i.e. a formal theory of an application domain akin to a domain model in ontology engineering, not to be confused with Scott's domain theory) in order to make generalizations or form concepts from training examples.[1] It is also linked with Encoding (memory) to help with Learning. [2]", "has_infobox": false, "num_categories": 1, "num_images": 1, "num_infobox_rows": 0, "num_references": 14, "num_sections": 5, "page_length_chars": 7039, "scraped_at": "2026-01-07T00:26:45.470801", "title": "Explanation-based learning", "url": "https://en.wikipedia.org/wiki/Explanation-based_learning"}
{"url_hash": "f986a679082e5a8f707da485aaf72384", "first_paragraph": "The exploration–exploitation dilemma, also known as the explore–exploit tradeoff, is a fundamental concept in decision-making that arises in many domains.[1][2] It is depicted as the balancing act between two opposing strategies. Exploitation involves choosing the best option based on current knowledge of the system (which may be incomplete or misleading), while exploration involves trying out new options that may lead to better outcomes in the future at the expense of an exploitation opportunit", "has_infobox": false, "num_categories": 5, "num_images": 26, "num_infobox_rows": 0, "num_references": 17, "num_sections": 7, "page_length_chars": 12424, "scraped_at": "2026-01-07T00:26:47.500224", "title": "Exploration–exploitation dilemma", "url": "https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma"}
{"url_hash": "0c4297acb6d19a768bdd031d21cf24c0", "first_paragraph": "Fairness in machine learning (ML) refers to the various attempts to correct algorithmic bias in automated decision processes based on ML models. Decisions made by such models after a learning process may be considered unfair if they were based on variables considered sensitive (e.g., gender, ethnicity, sexual orientation, or disability).", "has_infobox": false, "num_categories": 6, "num_images": 170, "num_infobox_rows": 0, "num_references": 55, "num_sections": 22, "page_length_chars": 48725, "scraped_at": "2026-01-07T00:26:50.543274", "title": "Fairness (machine learning)", "url": "https://en.wikipedia.org/wiki/Fairness_(machine_learning)"}
{"url_hash": "c5c6ed542fb04745a61515c3c6e9a103", "first_paragraph": "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a data set.[1] Choosing informative, discriminating, and independent features is crucial to producing effective algorithms for pattern recognition, classification, and regression tasks. Features are usually numeric, but other types such as strings and graphs are used in syntactic pattern recognition, after some pre-processing step such as one-hot encoding. The concept of \"features\" is", "has_infobox": false, "num_categories": 3, "num_images": 2, "num_infobox_rows": 0, "num_references": 7, "num_sections": 7, "page_length_chars": 8446, "scraped_at": "2026-01-07T00:26:52.237761", "title": "Feature (machine learning)", "url": "https://en.wikipedia.org/wiki/Feature_(machine_learning)"}
{"url_hash": "b35c2f718d44cad181d706f87c70fb2c", "first_paragraph": "Feature engineering is a preprocessing step in supervised machine learning and statistical modeling[1] which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability.[2][3][4]", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 40, "num_sections": 11, "page_length_chars": 16266, "scraped_at": "2026-01-07T00:26:54.017245", "title": "Feature engineering", "url": "https://en.wikipedia.org/wiki/Feature_engineering"}
{"url_hash": "11eee933bfb3aeaeff4c3fee5487ff19", "first_paragraph": "In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix.[1][2] It works by applying a hash function to the features and using their hash values as indices directly (after a modulo operation), rather than looking the indices up in an associative array. In addition to its use for encoding non-numeric values, feature hashing ca", "has_infobox": false, "num_categories": 2, "num_images": 94, "num_infobox_rows": 0, "num_references": 14, "num_sections": 11, "page_length_chars": 13049, "scraped_at": "2026-01-07T00:26:56.448912", "title": "Feature hashing", "url": "https://en.wikipedia.org/wiki/Feature_hashing"}
{"url_hash": "25c4bbbe4c545e982310e26d720cd6dc", "first_paragraph": "In machine learning (ML), feature learning or representation learning[2] is a set of techniques that allow a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.", "has_infobox": false, "num_categories": 1, "num_images": 2, "num_infobox_rows": 0, "num_references": 51, "num_sections": 22, "page_length_chars": 36413, "scraped_at": "2026-01-07T00:26:58.736767", "title": "Feature learning", "url": "https://en.wikipedia.org/wiki/Feature_learning"}
{"url_hash": "8d54017492c3231b091257164fd1dd89", "first_paragraph": "Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.", "has_infobox": false, "num_categories": 2, "num_images": 20, "num_infobox_rows": 0, "num_references": 5, "num_sections": 11, "page_length_chars": 5994, "scraped_at": "2026-01-07T00:27:01.178191", "title": "Feature scaling", "url": "https://en.wikipedia.org/wiki/Feature_scaling"}
{"url_hash": "c7e39f963bf5f73502fdcd4f1530d532", "first_paragraph": "Federated learning (also known as collaborative learning) is a machine learning technique in a setting where multiple entities (often called clients) collaboratively train a model while keeping their data decentralized,[1] rather than centrally stored. A defining characteristic of federated learning is data heterogeneity. Because client data is decentralized, data samples held by each client may not be independently and identically distributed.", "has_infobox": false, "num_categories": 2, "num_images": 25, "num_infobox_rows": 0, "num_references": 57, "num_sections": 32, "page_length_chars": 44345, "scraped_at": "2026-01-07T00:27:03.670836", "title": "Federated learning", "url": "https://en.wikipedia.org/wiki/Federated_learning"}
{"url_hash": "b3bae1923e18598f28673d119ec81ca8", "first_paragraph": "Fine-tuning (in deep learning) is the process of adapting a model trained for one task (the upstream task) to perform a different, usually more specific, task (the downstream task). It is considered a form of transfer learning, as it reuses knowledge learned from the original training objective.[1][2]", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 24, "num_sections": 9, "page_length_chars": 12177, "scraped_at": "2026-01-07T00:27:05.347532", "title": "Fine-tuning (deep learning)", "url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)"}
{"url_hash": "5ec7b383fddc1f6c39e4f53aa255eb31", "first_paragraph": "A flow-based generative model is a generative model used in machine learning that explicitly models a probability distribution by leveraging normalizing flow,[1][2][3] which is a statistical method using the change-of-variable law of probabilities to transform a simple distribution into a complex one.", "has_infobox": false, "num_categories": 3, "num_images": 353, "num_infobox_rows": 0, "num_references": 42, "num_sections": 19, "page_length_chars": 33416, "scraped_at": "2026-01-07T00:27:09.466337", "title": "Flow-based generative model", "url": "https://en.wikipedia.org/wiki/Flow-based_generative_model"}
{"url_hash": "ffa46579dff30f1b8d94162a65bcc6eb", "first_paragraph": "Flux is an open-source machine-learning software library and ecosystem written in Julia.[1][6] Its current stable release is v0.16.5[4] . It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design.[7] For example, GPU support is implemented transparently by CuArrays.jl.[8] This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings,", "has_infobox": true, "num_categories": 3, "num_images": 4, "num_infobox_rows": 11, "num_references": 25, "num_sections": 2, "page_length_chars": 6655, "scraped_at": "2026-01-07T00:27:11.762845", "title": "Flux (machine-learning framework)", "url": "https://en.wikipedia.org/wiki/Flux_(machine-learning_framework)"}
{"url_hash": "52d35f8808f4e52f9f33e469f08487b6", "first_paragraph": "Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. F", "has_infobox": false, "num_categories": 2, "num_images": 27, "num_infobox_rows": 0, "num_references": 13, "num_sections": 17, "page_length_chars": 24842, "scraped_at": "2026-01-07T00:27:13.903128", "title": "Force control", "url": "https://en.wikipedia.org/wiki/Force_control"}
{"url_hash": "812f4585f2c38965361e3ea384636f45", "first_paragraph": "In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices", "has_infobox": false, "num_categories": 6, "num_images": 53, "num_infobox_rows": 0, "num_references": 52, "num_sections": 20, "page_length_chars": 35398, "scraped_at": "2026-01-07T00:27:15.690974", "title": "Formal concept analysis", "url": "https://en.wikipedia.org/wiki/Formal_concept_analysis"}
{"url_hash": "bd117e6b6581833f121e984f1a066688", "first_paragraph": "", "has_infobox": false, "num_categories": 3, "num_images": 32, "num_infobox_rows": 0, "num_references": 13, "num_sections": 6, "page_length_chars": 12059, "scraped_at": "2026-01-07T00:27:17.610598", "title": "Generalized additive model for location, scale and shape", "url": "https://en.wikipedia.org/wiki/Generalized_additive_model_for_location,_scale_and_shape"}
{"url_hash": "651fc063313a0289e8537109ec96d2fc", "first_paragraph": "", "has_infobox": false, "num_categories": 9, "num_images": 15, "num_infobox_rows": 0, "num_references": 201, "num_sections": 31, "page_length_chars": 85140, "scraped_at": "2026-01-07T00:27:20.038850", "title": "Generative artificial intelligence", "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence"}
{"url_hash": "fddcfbbd35440d0c463f3998acec0b91", "first_paragraph": "Generative models, are a class of models frequently used for classification. In machine learning, it typically models the joint distribution of inputs and outputs, such as P(X,Y), or it models how inputs are distributed within each class, such as P(X∣Y) together with a class prior P(Y). Because it describes a full data-generating process, a generative model can be used to draw new samples that resemble the observed data. Generative models are used for density estimation, simulation, and learning", "has_infobox": false, "num_categories": 3, "num_images": 69, "num_infobox_rows": 0, "num_references": 16, "num_sections": 12, "page_length_chars": 15630, "scraped_at": "2026-01-07T00:27:22.744322", "title": "Generative model", "url": "https://en.wikipedia.org/wiki/Generative_model"}
{"url_hash": "4daceac686879712701a4236dbe8a224", "first_paragraph": "Geometric feature learning is a technique combining machine learning and computer vision to solve visual tasks. The main goal of this method is to find a set of representative features of geometric form to represent an object by collecting geometric features from images and learning them using efficient machine learning methods. Humans solve visual tasks and can give fast response to the environment by extracting perceptual information from what they see. Researchers simulate humans' ability of ", "has_infobox": false, "num_categories": 3, "num_images": 31, "num_infobox_rows": 0, "num_references": 9, "num_sections": 11, "page_length_chars": 9756, "scraped_at": "2026-01-07T00:27:25.712573", "title": "Geometric feature learning", "url": "https://en.wikipedia.org/wiki/Geometric_feature_learning"}
{"url_hash": "441c02aab14ebdf235a3e39d66538693", "first_paragraph": "", "has_infobox": false, "num_categories": 3, "num_images": 2, "num_infobox_rows": 0, "num_references": 368, "num_sections": 27, "page_length_chars": 200532, "scraped_at": "2026-01-07T00:27:28.059219", "title": "Glossary of artificial intelligence", "url": "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence"}
{"url_hash": "1d05037c4006cb45f178c5590ed77946", "first_paragraph": "Google Colaboratory, or Google Colab for short, is a free, cloud-based Jupyter Notebook environment provided by Google. It allows users to write and execute Python code through the browser, especially suited for machine learning, data analysis, and education. Google Colab provides an online integrated development environment (IDE) for Python that requires no setup and runs entirely in the cloud. It offers free access to computing resources, including GPUs and TPUs, making it popular among resear", "has_infobox": true, "num_categories": 8, "num_images": 3, "num_infobox_rows": 8, "num_references": 5, "num_sections": 4, "page_length_chars": 2736, "scraped_at": "2026-01-07T00:27:30.436961", "title": "Google Colab", "url": "https://en.wikipedia.org/wiki/Google_Colab"}
{"url_hash": "de28d04c6da3de64320810147acd3b74", "first_paragraph": "Google Research (also known as Research at Google) is the research division of Google, a subsidiary of Alphabet Inc.. According to its official website, Google Research publishes findings, releases open-source software, and applies research results within Google products and services as well as within the wider scientific community.[1]", "has_infobox": true, "num_categories": 6, "num_images": 4, "num_infobox_rows": 8, "num_references": 7, "num_sections": 4, "page_length_chars": 2881, "scraped_at": "2026-01-07T00:27:32.532074", "title": "Google Research", "url": "https://en.wikipedia.org/wiki/Google_Research"}
{"url_hash": "d3e4fa3bd23b6702c5965b3689981fdb", "first_paragraph": "Granular computing is an emerging computing paradigm of information processing that concerns the processing of complex information entities called \"information granules\", which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherenc", "has_infobox": false, "num_categories": 2, "num_images": 126, "num_infobox_rows": 0, "num_references": 0, "num_sections": 8, "page_length_chars": 30844, "scraped_at": "2026-01-07T00:27:35.981742", "title": "Granular computing", "url": "https://en.wikipedia.org/wiki/Granular_computing"}
{"url_hash": "5f028975c1e73a30a33656a3f09902b2", "first_paragraph": "In machine learning, grokking, or delayed generalization, is a phenomenon observed in some settings where a model abruptly transitions from overfitting (performing well only on training data) to generalizing (performing well on both training and test data), after many training iterations with little or no improvement on the held-out data.[2] This contrasts with what is typically observed in machine learning, where generalization occurs gradually alongside improved performance on training data.[3", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 18, "num_sections": 3, "page_length_chars": 7173, "scraped_at": "2026-01-07T00:27:38.197888", "title": "Grokking (machine learning)", "url": "https://en.wikipedia.org/wiki/Grokking_(machine_learning)"}
{"url_hash": "17200d53399897e4bfc8ebd718c5bf73", "first_paragraph": "", "has_infobox": true, "num_categories": 7, "num_images": 3, "num_infobox_rows": 7, "num_references": 11, "num_sections": 5, "page_length_chars": 4459, "scraped_at": "2026-01-07T00:27:40.283884", "title": "H (company)", "url": "https://en.wikipedia.org/wiki/H_(company)"}
{"url_hash": "580141589f08fcee062495a569c3d5ed", "first_paragraph": "", "has_infobox": true, "num_categories": 6, "num_images": 5, "num_infobox_rows": 14, "num_references": 10, "num_sections": 6, "page_length_chars": 6158, "scraped_at": "2026-01-07T00:27:42.587568", "title": "H2O (software)", "url": "https://en.wikipedia.org/wiki/H2O_(software)"}
{"url_hash": "6ed12b9e98d2f930c5649fd99a7c8f6f", "first_paragraph": "", "has_infobox": false, "num_categories": 14, "num_images": 7, "num_infobox_rows": 0, "num_references": 119, "num_sections": 18, "page_length_chars": 63296, "scraped_at": "2026-01-07T00:27:45.208049", "title": "Hallucination (artificial intelligence)", "url": "https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)"}
{"url_hash": "751523b47e1de171a847df444ac23800", "first_paragraph": "In artificial neural networks, a hidden layer is a layer of artificial neurons that is neither an input layer nor an output layer. The simplest examples appear in multilayer perceptrons (MLP), as illustrated in the diagram.[1]", "has_infobox": false, "num_categories": 4, "num_images": 2, "num_infobox_rows": 0, "num_references": 1, "num_sections": 1, "page_length_chars": 926, "scraped_at": "2026-01-07T00:27:47.739017", "title": "Hidden layer", "url": "https://en.wikipedia.org/wiki/Hidden_layer"}
{"url_hash": "a3ab0a4e33b899c6683e205ad4da0043", "first_paragraph": "The Hierarchical navigable small world (HNSW) algorithm is a graph-based approximate nearest neighbor search technique used in many vector databases.[1] Nearest neighbor search without an index involves computing the distance from the query to each point in the database, which, for large datasets, is computationally prohibitive. For high-dimensional data, tree-based exact vector search techniques such as the k-d tree and R-tree do not perform well enough because of the curse of dimensionality. T", "has_infobox": false, "num_categories": 8, "num_images": 5, "num_infobox_rows": 0, "num_references": 19, "num_sections": 2, "page_length_chars": 6312, "scraped_at": "2026-01-07T00:27:49.833655", "title": "Hierarchical navigable small world", "url": "https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world"}
{"url_hash": "ce17519ed4be5c30ecb1b48e7b06946c", "first_paragraph": "Hierarchical Risk Parity (HRP) is an advanced investment portfolio optimization framework developed in 2016 by Marcos López de Prado at Guggenheim Partners and Cornell University. HRP is a probabilistic graph-based alternative to the prevailing mean-variance optimization (MVO) framework developed by Harry Markowitz in 1952, and for which he received the Nobel Prize in economic sciences.[1] HRP algorithms apply discrete mathematics and machine learning techniques to create diversified and robust ", "has_infobox": false, "num_categories": 3, "num_images": 46, "num_infobox_rows": 0, "num_references": 25, "num_sections": 10, "page_length_chars": 24715, "scraped_at": "2026-01-07T00:27:51.988045", "title": "Hierarchical Risk Parity", "url": "https://en.wikipedia.org/wiki/Hierarchical_Risk_Parity"}
{"url_hash": "b885c547a0366b4d6d5f00d48ebcb539", "first_paragraph": "In machine learning, the Highway Network was the first working very deep feedforward neural network with hundreds of layers, much deeper than previous neural networks.[1][2][3] It uses skip connections modulated by learned gating mechanisms to regulate information flow, inspired by long short-term memory (LSTM) recurrent neural networks.[4][5] The advantage of the Highway Network over other deep learning architectures is its ability to overcome or partially prevent the vanishing gradient problem", "has_infobox": false, "num_categories": 3, "num_images": 10, "num_infobox_rows": 0, "num_references": 18, "num_sections": 4, "page_length_chars": 11128, "scraped_at": "2026-01-07T00:27:54.289906", "title": "Highway network", "url": "https://en.wikipedia.org/wiki/Highway_network"}
{"url_hash": "edd9b7014d27d33a87f88d2d9ef81896", "first_paragraph": "Hugging Face, Inc. is an American company based in New York City that develops computation tools for building applications using machine learning. Its transformers library built for natural language processing applications and its platform allows users to share machine learning models and datasets and showcase their work.", "has_infobox": true, "num_categories": 5, "num_images": 5, "num_infobox_rows": 11, "num_references": 14, "num_sections": 4, "page_length_chars": 5145, "scraped_at": "2026-01-07T00:27:56.021998", "title": "Hugging Face", "url": "https://en.wikipedia.org/wiki/Hugging_Face"}
{"url_hash": "ff97bd48fe1c02b12d660b1c91c9396a", "first_paragraph": "Human-in-the-loop (HITL) is used in multiple contexts. It can be defined as a model requiring human interaction.[1][2] HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL, along with the related human-on-the-loop are also used in relation to lethal autonomous weapons.[3] Further, HITL is used in the context of machine learning.[4]", "has_infobox": false, "num_categories": 6, "num_images": 1, "num_infobox_rows": 0, "num_references": 9, "num_sections": 9, "page_length_chars": 6967, "scraped_at": "2026-01-07T00:27:58.503770", "title": "Human-in-the-loop", "url": "https://en.wikipedia.org/wiki/Human-in-the-loop"}
{"url_hash": "a1d64553eda831d6793722cc7138aef1", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 14, "num_sections": 9, "page_length_chars": 11546, "scraped_at": "2026-01-07T00:28:00.775889", "title": "Hyperparameter (machine learning)", "url": "https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)"}
{"url_hash": "23f73873a73a296727b75e4f32b45884", "first_paragraph": "In machine learning, hyperparameter optimization[1] or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts.[2][3]", "has_infobox": false, "num_categories": 3, "num_images": 7, "num_infobox_rows": 0, "num_references": 37, "num_sections": 12, "page_length_chars": 19352, "scraped_at": "2026-01-07T00:28:02.885061", "title": "Hyperparameter optimization", "url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization"}
{"url_hash": "69807fdca0fbb5727fb654d3e6a2e6aa", "first_paragraph": "Prompt engineering is the process of structuring or crafting an instruction in order to produce better outputs from a generative artificial intelligence (AI) model. It typically involves designing clear queries, adding relevant context, and refining wording to guide the model toward more accurate, useful, and consistent responses.[1]", "has_infobox": false, "num_categories": 7, "num_images": 27, "num_infobox_rows": 0, "num_references": 69, "num_sections": 19, "page_length_chars": 34037, "scraped_at": "2026-01-07T00:28:05.507738", "title": "In-context learning (natural language processing)", "url": "https://en.wikipedia.org/wiki/In-context_learning_(natural_language_processing)"}
{"url_hash": "58a4791af27a210aee7934a9b7ac0b7b", "first_paragraph": "An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.", "has_infobox": false, "num_categories": 6, "num_images": 3, "num_infobox_rows": 0, "num_references": 0, "num_sections": 2, "page_length_chars": 2556, "scraped_at": "2026-01-07T00:28:07.240452", "title": "Inauthentic text", "url": "https://en.wikipedia.org/wiki/Inauthentic_text"}
{"url_hash": "b13515bc11d93bcd9b7f9a8a107725d4", "first_paragraph": "The Inception Score (IS) is an algorithm used to assess the quality of images created by a generative image model such as a generative adversarial network (GAN).[1] The score is calculated based on the output of a separate, pretrained Inception v3 image classification model applied to a sample of (typically around 30,000) images generated by the generative model. The Inception Score is maximized when the following conditions are true:", "has_infobox": false, "num_categories": 2, "num_images": 45, "num_infobox_rows": 0, "num_references": 3, "num_sections": 3, "page_length_chars": 4214, "scraped_at": "2026-01-07T00:28:09.920300", "title": "Inception score", "url": "https://en.wikipedia.org/wiki/Inception_score"}
{"url_hash": "3b298afdd26bc9848f48850fa3ed1f51", "first_paragraph": "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered.[1] Inductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g., step-functions in decision trees instead of continuous functions in linear regression models). Learning involves searching a space of solutions for a solution that provides a good explanation of the data. Ho", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 5, "num_sections": 4, "page_length_chars": 5168, "scraped_at": "2026-01-07T00:28:11.838618", "title": "Inductive bias", "url": "https://en.wikipedia.org/wiki/Inductive_bias"}
{"url_hash": "1ae77a1ea9ce8d80388518242518df8d", "first_paragraph": "Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world.", "has_infobox": false, "num_categories": 5, "num_images": 137, "num_infobox_rows": 0, "num_references": 17, "num_sections": 31, "page_length_chars": 32542, "scraped_at": "2026-01-07T00:28:14.998855", "title": "Inductive probability", "url": "https://en.wikipedia.org/wiki/Inductive_probability"}
{"url_hash": "69d5262a06126ec1a0a045ff4a9f8e3e", "first_paragraph": "Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 35, "num_sections": 7, "page_length_chars": 17804, "scraped_at": "2026-01-07T00:28:16.663568", "title": "Inductive programming", "url": "https://en.wikipedia.org/wiki/Inductive_programming"}
{"url_hash": "1c8eb10a07718214d7a796152462a628", "first_paragraph": "Inferential Theory of Learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been continuously developed by Ryszard S. Michalski, starting in the 1980s. The first known publication of ITL was in 1983.[1] In the ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. The results of learning need to be stored. Stored information will later be used by the learner for future inferenc", "has_infobox": false, "num_categories": 5, "num_images": 3, "num_infobox_rows": 0, "num_references": 4, "num_sections": 3, "page_length_chars": 3048, "scraped_at": "2026-01-07T00:28:19.127995", "title": "Inferential theory of learning", "url": "https://en.wikipedia.org/wiki/Inferential_theory_of_learning"}
{"url_hash": "43a1500d35dca5e55e7d7b518f9db84a", "first_paragraph": "Instance selection (or dataset reduction, or dataset condensation) is an important data pre-processing step that can be applied in many machine learning (or data mining) tasks.[1] Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying lear", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 11, "num_sections": 2, "page_length_chars": 6106, "scraped_at": "2026-01-07T00:28:21.537464", "title": "Instance selection", "url": "https://en.wikipedia.org/wiki/Instance_selection"}
{"url_hash": "d7911f618de8e51173339f03c0e21aa3", "first_paragraph": "In machine learning, instance-based learning (sometimes called memory-based learning[1]) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. Because computation is postponed until a new instance is observed, these algorithms are sometimes referred to as \"lazy.\"[2]", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 4, "num_sections": 2, "page_length_chars": 2060, "scraped_at": "2026-01-07T00:28:23.888742", "title": "Instance-based learning", "url": "https://en.wikipedia.org/wiki/Instance-based_learning"}
{"url_hash": "8a1a7569ed43d3e28b3fe92f4dfd3d64", "first_paragraph": "Intelligent automation (IA), or intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA).[1] Companies use intelligent automation to cut costs and streamline tasks by using artificial-intelligence-powered robotic software to mitigate repetitive tasks.[1] As it accumulates data, the system learns in an effort to improve its efficiency.[2] Intelligent automation applications consist of but are not limited t", "has_infobox": false, "num_categories": 6, "num_images": 1, "num_infobox_rows": 0, "num_references": 18, "num_sections": 6, "page_length_chars": 7881, "scraped_at": "2026-01-07T00:28:26.253020", "title": "Intelligent automation", "url": "https://en.wikipedia.org/wiki/Intelligent_automation"}
{"url_hash": "c8e88d70142f680530ca99333f3756f5", "first_paragraph": "In the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is proportional to the identity matrix.", "has_infobox": false, "num_categories": 4, "num_images": 15, "num_infobox_rows": 0, "num_references": 0, "num_sections": 3, "page_length_chars": 1182, "scraped_at": "2026-01-07T00:28:28.517387", "title": "Isotropic position", "url": "https://en.wikipedia.org/wiki/Isotropic_position"}
{"url_hash": "c0423f6fe8afef0553c564fffb298b57", "first_paragraph": "JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning. It is developed by Google with contributions from Nvidia and other community contributors.[1][2][3]", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 9, "num_sections": 4, "page_length_chars": 3520, "scraped_at": "2026-01-07T00:28:30.143285", "title": "JAX (software)", "url": "https://en.wikipedia.org/wiki/JAX_(software)"}
{"url_hash": "0986ff6f540bccf40ef2dfe3f7e45775", "first_paragraph": "", "has_infobox": true, "num_categories": 4, "num_images": 1, "num_infobox_rows": 17, "num_references": 3, "num_sections": 4, "page_length_chars": 2892, "scraped_at": "2026-01-07T00:28:32.038137", "title": "Journal of Machine Learning Research", "url": "https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research"}
{"url_hash": "facd702bc6b81f4193c86c05d4fcd518", "first_paragraph": "In statistics, kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights. KDE answers a fundamental data smoothing problem where inferences about the population are made based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel", "has_infobox": false, "num_categories": 3, "num_images": 49, "num_infobox_rows": 0, "num_references": 41, "num_sections": 11, "page_length_chars": 25938, "scraped_at": "2026-01-07T00:28:34.024050", "title": "Kernel density estimation", "url": "https://en.wikipedia.org/wiki/Kernel_density_estimation"}
{"url_hash": "89d2d866dd80b8897d3452697f9bfbda", "first_paragraph": "In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS).[1] A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, w", "has_infobox": false, "num_categories": 2, "num_images": 333, "num_infobox_rows": 0, "num_references": 26, "num_sections": 27, "page_length_chars": 32884, "scraped_at": "2026-01-07T00:28:38.389841", "title": "Kernel embedding of distributions", "url": "https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions"}
{"url_hash": "a03128a68ed6b1bdfa7bac154e4fca27", "first_paragraph": "In representation learning, knowledge graph embedding (KGE), also called knowledge representation learning (KRL), or multi-relation learning,[1] is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.[1][2][3] Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and r", "has_infobox": false, "num_categories": 4, "num_images": 89, "num_infobox_rows": 0, "num_references": 44, "num_sections": 19, "page_length_chars": 40185, "scraped_at": "2026-01-07T00:28:41.345038", "title": "Knowledge graph embedding", "url": "https://en.wikipedia.org/wiki/Knowledge_graph_embedding"}
{"url_hash": "21f3d04d0f0fab7371edcf575abe578c", "first_paragraph": "Knowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation).", "has_infobox": false, "num_categories": 3, "num_images": 0, "num_infobox_rows": 0, "num_references": 0, "num_sections": 3, "page_length_chars": 3867, "scraped_at": "2026-01-07T00:28:43.016931", "title": "Knowledge integration", "url": "https://en.wikipedia.org/wiki/Knowledge_integration"}
{"url_hash": "728ecf413176b71ef54db320062e0d0d", "first_paragraph": "Kolmogorov–Arnold Networks (KANs) are a type of artificial neural network architecture inspired by the Kolmogorov–Arnold representation theorem, also known as the superposition theorem. Unlike traditional multilayer perceptrons (MLPs), which rely on fixed activation functions and linear weights, KANs replace each weight with a learnable univariate function, often represented using splines.[1][2][3]", "has_infobox": false, "num_categories": 3, "num_images": 33, "num_infobox_rows": 0, "num_references": 37, "num_sections": 8, "page_length_chars": 13894, "scraped_at": "2026-01-07T00:28:45.566112", "title": "Kolmogorov-Arnold Networks", "url": "https://en.wikipedia.org/wiki/Kolmogorov-Arnold_Networks"}
{"url_hash": "273b3df8f9a92f3a7bc0e80dbde43392", "first_paragraph": "Labeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of it with informative tags called judgments. For example, a data label might indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, or whether a dot in an X-ray is a tum", "has_infobox": false, "num_categories": 1, "num_images": 4, "num_infobox_rows": 0, "num_references": 8, "num_sections": 8, "page_length_chars": 6398, "scraped_at": "2026-01-07T00:28:47.515245", "title": "Labeled data", "url": "https://en.wikipedia.org/wiki/Labeled_data"}
{"url_hash": "974c67a7d7bd1aea451940f297571c85", "first_paragraph": "(Not to be confused with the lazy learning regime, see Neural tangent kernel).", "has_infobox": false, "num_categories": 1, "num_images": 0, "num_infobox_rows": 0, "num_references": 7, "num_sections": 5, "page_length_chars": 7340, "scraped_at": "2026-01-07T00:28:49.072670", "title": "Lazy learning", "url": "https://en.wikipedia.org/wiki/Lazy_learning"}
{"url_hash": "6feeb11d89964d421ee1413d6862e91a", "first_paragraph": "In statistics and machine learning, leakage (also known as data leakage or target leakage) refers to the use of information during model training that would not be available at prediction time. This results in overly optimistic performance estimates, as the model appears to perform better during evaluation than it actually would in a production environment.[1]", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 11, "num_sections": 6, "page_length_chars": 7630, "scraped_at": "2026-01-07T00:28:51.009738", "title": "Leakage (machine learning)", "url": "https://en.wikipedia.org/wiki/Leakage_(machine_learning)"}
{"url_hash": "ebbd9d92a8991010d7fcf2cd36452a01", "first_paragraph": "A learnable function is a mathematical function that can be learned from data, typically through a machine learning algorithm, to minimize errors and perform a specific task. In the context of statistical learning theory, this refers to a function class where an algorithm can identify a specific function that best approximates the underlying pattern or distribution within the data.[1]", "has_infobox": false, "num_categories": 4, "num_images": 34, "num_infobox_rows": 0, "num_references": 9, "num_sections": 13, "page_length_chars": 9612, "scraped_at": "2026-01-07T00:28:53.568916", "title": "Learnable function", "url": "https://en.wikipedia.org/wiki/Learnable_function"}
{"url_hash": "24e6d3ecf4b9388b401cb2858be2a6c1", "first_paragraph": "In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.", "has_infobox": false, "num_categories": 1, "num_images": 66, "num_infobox_rows": 0, "num_references": 5, "num_sections": 7, "page_length_chars": 5932, "scraped_at": "2026-01-07T00:28:56.069331", "title": "Learnable function class", "url": "https://en.wikipedia.org/wiki/Learnable_function_class"}
{"url_hash": "59f84de7a08ccce3d2982ffa34b72a9f", "first_paragraph": "A learning automaton is one type of machine learning algorithm studied since 1970s. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and a Markov decision process (MDP) is used.", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 5, "num_sections": 6, "page_length_chars": 5043, "scraped_at": "2026-01-07T00:28:57.594519", "title": "Learning automaton", "url": "https://en.wikipedia.org/wiki/Learning_automaton"}
{"url_hash": "8c0823d88fb6574839db7652281497ca", "first_paragraph": "In machine learning (ML), a learning curve (or training curve) is a graphical representation that shows how a model's performance on a training set (and usually a validation set) changes with the number of training iterations (epochs) or the amount of training data.[1] Typically, the number of training epochs or training set size is plotted on the x-axis, and the value of the loss function (and possibly some other metric such as the cross-validation score) on the y-axis.", "has_infobox": false, "num_categories": 2, "num_images": 15, "num_infobox_rows": 0, "num_references": 7, "num_sections": 5, "page_length_chars": 4311, "scraped_at": "2026-01-07T00:28:59.368168", "title": "Learning curve (machine learning)", "url": "https://en.wikipedia.org/wiki/Learning_curve_(machine_learning)"}
{"url_hash": "ad05fc7983ad2e5d2cd0de306f57b9bc", "first_paragraph": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.[1] Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model \"learns\". In the adaptive control literature, the learning rate is commonly referred to as gain.[2]", "has_infobox": false, "num_categories": 3, "num_images": 14, "num_infobox_rows": 0, "num_references": 10, "num_sections": 6, "page_length_chars": 7028, "scraped_at": "2026-01-07T00:29:01.417173", "title": "Learning rate", "url": "https://en.wikipedia.org/wiki/Learning_rate"}
{"url_hash": "433a9e4bbcc41dd2548e7f4f5ea60644", "first_paragraph": "Learning to rank[1] (LTR) or machine-learned ranking (MLR) is the application of machine learning, often supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval and recommender systems.[2] Training data may, for example, consist of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") ", "has_infobox": false, "num_categories": 3, "num_images": 19, "num_infobox_rows": 0, "num_references": 65, "num_sections": 16, "page_length_chars": 34527, "scraped_at": "2026-01-07T00:29:03.885426", "title": "Learning to rank", "url": "https://en.wikipedia.org/wiki/Learning_to_rank"}
{"url_hash": "273cea8e172de1380eb10bdeff73cf08", "first_paragraph": "In probability theory and related fields, the life-time of correlation measures the timespan over which there is appreciable autocorrelation or cross-correlation in stochastic processes.", "has_infobox": false, "num_categories": 4, "num_images": 3, "num_infobox_rows": 0, "num_references": 1, "num_sections": 2, "page_length_chars": 1713, "scraped_at": "2026-01-07T00:29:06.101594", "title": "Life-time of correlation", "url": "https://en.wikipedia.org/wiki/Life-time_of_correlation"}
{"url_hash": "1c5107533442d701147912c8194f95b7", "first_paragraph": "In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable.[1] This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression,[2] perceptrons,[3] support vector ma", "has_infobox": false, "num_categories": 2, "num_images": 29, "num_infobox_rows": 0, "num_references": 7, "num_sections": 7, "page_length_chars": 9889, "scraped_at": "2026-01-07T00:29:08.155847", "title": "Linear predictor function", "url": "https://en.wikipedia.org/wiki/Linear_predictor_function"}
{"url_hash": "05f2d80d58f0b743e59d593bb3c08c5d", "first_paragraph": "In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean", "has_infobox": false, "num_categories": 3, "num_images": 67, "num_infobox_rows": 0, "num_references": 9, "num_sections": 8, "page_length_chars": 7925, "scraped_at": "2026-01-07T00:29:10.686800", "title": "Linear separability", "url": "https://en.wikipedia.org/wiki/Linear_separability"}
{"url_hash": "1df686f30a8376abc2206632982606fa", "first_paragraph": "LLM-as-a-Judge (also known as LLM-based evaluation, LLM judges or LLMs-as-judges) is a family of techniques in natural language processing in which large language models (LLMs) are used as automated evaluators of texts or other model outputs. Instead of relying only on human annotators, an LLM judge is prompted or fine tuned to assign scores, labels or preferences according to specified criteria such as usefulness, factuality, safety or style.[1][2]", "has_infobox": false, "num_categories": 3, "num_images": 2, "num_infobox_rows": 0, "num_references": 24, "num_sections": 17, "page_length_chars": 24264, "scraped_at": "2026-01-07T00:29:12.463870", "title": "LLM-as-a-Judge", "url": "https://en.wikipedia.org/wiki/LLM-as-a-Judge"}
{"url_hash": "d6dfde5fff67c93eb4c1694ebfc1069c", "first_paragraph": "In machine learning, local case-control sampling [1] is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of an (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most \"surprising\" samples. In practice, the pilot may come ", "has_infobox": false, "num_categories": 2, "num_images": 28, "num_infobox_rows": 0, "num_references": 1, "num_sections": 6, "page_length_chars": 4223, "scraped_at": "2026-01-07T00:29:14.433584", "title": "Local case-control sampling", "url": "https://en.wikipedia.org/wiki/Local_case-control_sampling"}
{"url_hash": "cc763a3725f71747949a73953ad6b8ed", "first_paragraph": "In machine learning, the lottery ticket hypothesis is that artificial neural networks with random weights can contain a subnetwork which (entirely by chance) can be tuned to a similar performance as tuning the whole network.[1]", "has_infobox": false, "num_categories": 3, "num_images": 13, "num_infobox_rows": 0, "num_references": 3, "num_sections": 3, "page_length_chars": 3414, "scraped_at": "2026-01-07T00:29:16.916891", "title": "Lottery ticket hypothesis", "url": "https://en.wikipedia.org/wiki/Lottery_ticket_hypothesis"}
{"url_hash": "740a994c457467d62427e2b579a72285", "first_paragraph": "Lyra is a lossy audio codec developed by Google that is designed for compressing speech at very low bitrates. Unlike most other audio formats, it compresses data using a machine learning-based algorithm.", "has_infobox": true, "num_categories": 6, "num_images": 2, "num_infobox_rows": 8, "num_references": 7, "num_sections": 9, "page_length_chars": 7803, "scraped_at": "2026-01-07T00:29:18.562785", "title": "Lyra (codec)", "url": "https://en.wikipedia.org/wiki/Lyra_(codec)"}
{"url_hash": "ba4a12d5a60ca7132f61dc274023d4f1", "first_paragraph": "In machine learning and computer vision, M-theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-theory, HMAX, achieved human-level performance.[1]", "has_infobox": false, "num_categories": 3, "num_images": 85, "num_infobox_rows": 0, "num_references": 16, "num_sections": 14, "page_length_chars": 22649, "scraped_at": "2026-01-07T00:29:21.263900", "title": "M-theory (learning framework)", "url": "https://en.wikipedia.org/wiki/M-theory_(learning_framework)"}
{"url_hash": "27721329792e0278fe13787f13d45840", "first_paragraph": "Machine Learning is a peer-reviewed scientific journal, published since 1986.", "has_infobox": true, "num_categories": 7, "num_images": 5, "num_infobox_rows": 13, "num_references": 8, "num_sections": 3, "page_length_chars": 4806, "scraped_at": "2026-01-07T00:29:23.214738", "title": "Machine Learning (journal)", "url": "https://en.wikipedia.org/wiki/Machine_Learning_(journal)"}
{"url_hash": "0ff217bfca6740a56c1698f7b1d358f7", "first_paragraph": "Machine Learning and Knowledge Extraction (MAKE) is a peer-reviewed open-access scientific journal covering research on machine learning, knowledge extraction and related areas of data-driven artificial intelligence. It is published by MDPI and was launched in 2019 with Andreas Holzinger as founding Editor-in-Chief.", "has_infobox": true, "num_categories": 7, "num_images": 1, "num_infobox_rows": 16, "num_references": 5, "num_sections": 3, "page_length_chars": 1834, "scraped_at": "2026-01-07T00:29:25.033620", "title": "Machine Learning and Knowledge Extraction", "url": "https://en.wikipedia.org/wiki/Machine_Learning_and_Knowledge_Extraction"}
{"url_hash": "762caf8acf2669bb0b74ea6e4ee7d71c", "first_paragraph": "Machine learning control (MLC) is a subfield of machine learning, intelligent control, and control theory which aims to solve optimal control problems with machine learning methods. Key applications are complex nonlinear systems for which linear control theory methods are not applicable.", "has_infobox": false, "num_categories": 3, "num_images": 10, "num_infobox_rows": 0, "num_references": 12, "num_sections": 6, "page_length_chars": 6345, "scraped_at": "2026-01-07T00:29:26.775232", "title": "Machine learning control", "url": "https://en.wikipedia.org/wiki/Machine_learning_control"}
{"url_hash": "d5ec4af63e9b8dc58cfeea658e348087", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 39, "num_infobox_rows": 0, "num_references": 97, "num_sections": 25, "page_length_chars": 62262, "scraped_at": "2026-01-07T00:29:28.999740", "title": "Machine learning in bioinformatics", "url": "https://en.wikipedia.org/wiki/Machine_learning_in_bioinformatics"}
{"url_hash": "c36894eb9ff1755893995191a5ab3b70", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 5, "num_infobox_rows": 0, "num_references": 49, "num_sections": 16, "page_length_chars": 40706, "scraped_at": "2026-01-07T00:29:32.279587", "title": "Machine learning in earth sciences", "url": "https://en.wikipedia.org/wiki/Machine_learning_in_earth_sciences"}
{"url_hash": "eb87665b89561b156f33e004c9e9e3e4", "first_paragraph": "Applying machine learning (ML) (including deep learning) methods to the study of quantum systems is an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement.[1] Other examples include learning Hamiltonians,[2][3] learning quantum phase transitions,[4][5] and automatically generating new quantum experiments.[6][7][8][9] ML is effective at processing large amounts of experimental or calculated data in order to char", "has_infobox": false, "num_categories": 4, "num_images": 5, "num_infobox_rows": 0, "num_references": 43, "num_sections": 9, "page_length_chars": 17449, "scraped_at": "2026-01-07T00:29:34.875363", "title": "Machine learning in physics", "url": "https://en.wikipedia.org/wiki/Machine_learning_in_physics"}
{"url_hash": "3da9dcd4595b94259c3548e0774e065c", "first_paragraph": "Artificial intelligence and machine learning techniques are used in video games for a wide variety of applications such as non-player character (NPC) control, procedural content generation (PCG) and deep learning-based content generation. Machine learning is a subset of artificial intelligence that uses historical data to build predictive and analytical models. This is in sharp contrast to traditional methods of artificial intelligence such as search trees and expert systems.", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 39, "num_sections": 30, "page_length_chars": 28760, "scraped_at": "2026-01-07T00:29:37.391804", "title": "Machine learning in video games", "url": "https://en.wikipedia.org/wiki/Machine_learning_in_video_games"}
{"url_hash": "0d6693a8d4c14d55a6855c0815c18016", "first_paragraph": "Machine unlearning is a branch of machine learning focused on removing specific undesired element, such as private data, wrong or manipulated training data, outdated information, copyrighted material, harmful content, dangerous abilities, or misinformation, without needing to rebuild models from the ground up.", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 4, "num_sections": 4, "page_length_chars": 4904, "scraped_at": "2026-01-07T00:29:39.880754", "title": "Machine unlearning", "url": "https://en.wikipedia.org/wiki/Machine_unlearning"}
{"url_hash": "b7aca0021e69e984ef1edf26a984e5cb", "first_paragraph": "Machine-learned interatomic potentials (MLIPs), or simply machine learning potentials (MLPs), are interatomic potentials constructed using machine learning. Beginning in the 1990s, researchers have employed such programs to construct interatomic potentials by mapping atomic structures to their potential energies. These potentials are referred to as MLIPs or MLPs.", "has_infobox": false, "num_categories": 3, "num_images": 0, "num_infobox_rows": 0, "num_references": 16, "num_sections": 2, "page_length_chars": 8460, "scraped_at": "2026-01-07T00:29:41.526861", "title": "Machine-learned interatomic potential", "url": "https://en.wikipedia.org/wiki/Machine-learned_interatomic_potential"}
{"url_hash": "faf246f484bed352065e2bb5afdffc1a", "first_paragraph": "The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space.[1][2][3][4] As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, linked to the local coordinate system of the underlying manifold. It is suggested that this principle underpin", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 7, "num_sections": 4, "page_length_chars": 4573, "scraped_at": "2026-01-07T00:29:43.871757", "title": "Manifold hypothesis", "url": "https://en.wikipedia.org/wiki/Manifold_hypothesis"}
{"url_hash": "6282de557ea7a66fb24bf81859374706", "first_paragraph": "In machine learning, manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathemat", "has_infobox": false, "num_categories": 1, "num_images": 95, "num_infobox_rows": 0, "num_references": 28, "num_sections": 14, "page_length_chars": 18584, "scraped_at": "2026-01-07T00:29:46.795921", "title": "Manifold regularization", "url": "https://en.wikipedia.org/wiki/Manifold_regularization"}
{"url_hash": "18c2fcdd2e2c3cfc53b5f3dd7dfe8550", "first_paragraph": "The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.", "has_infobox": true, "num_categories": 7, "num_images": 1, "num_infobox_rows": 11, "num_references": 8, "num_sections": 6, "page_length_chars": 3544, "scraped_at": "2026-01-07T00:29:49.349118", "title": "The Master Algorithm", "url": "https://en.wikipedia.org/wiki/The_Master_Algorithm"}
{"url_hash": "709e81c9ea129b99ac2b06d8674300b6", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 9, "num_infobox_rows": 0, "num_references": 32, "num_sections": 11, "page_length_chars": 16952, "scraped_at": "2026-01-07T00:29:51.683440", "title": "Matchbox Educable Noughts and Crosses Engine", "url": "https://en.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine"}
{"url_hash": "27bfaeae16198e2813c3aff205b539cc", "first_paragraph": "In the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over\nto find a vector\nthat is a stable solution to the regression problem. When the system is described by a matr", "has_infobox": false, "num_categories": 3, "num_images": 96, "num_infobox_rows": 0, "num_references": 9, "num_sections": 10, "page_length_chars": 10421, "scraped_at": "2026-01-07T00:29:53.722416", "title": "Matrix regularization", "url": "https://en.wikipedia.org/wiki/Matrix_regularization"}
{"url_hash": "4ec55b1db9e8321651e757f43355c3e1", "first_paragraph": "MAUVE is a metric for automatically evaluating the quality of open-ended text generation and other generative models. Developed by researchers at the University of Washington, Allen Institute for AI, and Stanford University, it was first introduced at NeurIPS 2021, where it received and Outstanding Paper Award.[1][2]", "has_infobox": false, "num_categories": 4, "num_images": 11, "num_infobox_rows": 0, "num_references": 5, "num_sections": 8, "page_length_chars": 5821, "scraped_at": "2026-01-07T00:29:55.823939", "title": "MAUVE (metric)", "url": "https://en.wikipedia.org/wiki/MAUVE_(metric)"}
{"url_hash": "d8a4efdbd2f0a37ddebe9733bee10bbf", "first_paragraph": "Maximum inner-product search (MIPS) is a search problem, with a corresponding class of search algorithms which attempt to maximise the inner product between a query and the data items to be retrieved. MIPS algorithms are used in a wide variety of big data applications, including recommendation algorithms and machine learning.[1]", "has_infobox": false, "num_categories": 3, "num_images": 5, "num_infobox_rows": 0, "num_references": 5, "num_sections": 2, "page_length_chars": 2217, "scraped_at": "2026-01-07T00:29:57.821405", "title": "Maximum inner-product search", "url": "https://en.wikipedia.org/wiki/Maximum_inner-product_search"}
{"url_hash": "6dda30e5dfc5dca767fc182b84b3a43d", "first_paragraph": "Mechanistic interpretability (often abbreviated as mech interp, mechinterp, or MI) is a subfield of research within explainable artificial intelligence that aims to understand the internal workings of neural networks by analyzing the mechanisms present in their computations. The approach seeks to analyze neural networks in a manner similar to how binary computer programs can be reverse-engineered to understand their functions.", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 8, "num_sections": 6, "page_length_chars": 3428, "scraped_at": "2026-01-07T00:29:59.919945", "title": "Mechanistic interpretability", "url": "https://en.wikipedia.org/wiki/Mechanistic_interpretability"}
{"url_hash": "52f7d69eb12ce81ce927f55f56ffb822", "first_paragraph": "Meta-labeling, also known as corrective AI, is a machine learning (ML) technique utilized in quantitative finance to enhance the performance of investment and trading strategies, developed in 2017 by Marcos López de Prado at Guggenheim Partners and Cornell University.[1] The core idea is to separate the decision of trade direction (side) from the decision of trade sizing, addressing the inefficiencies of simultaneously learning both side and size predictions. The side decision involves forecasti", "has_infobox": false, "num_categories": 1, "num_images": 10, "num_infobox_rows": 0, "num_references": 8, "num_sections": 17, "page_length_chars": 17666, "scraped_at": "2026-01-07T00:30:02.627470", "title": "Meta-Labeling", "url": "https://en.wikipedia.org/wiki/Meta-Labeling"}
{"url_hash": "5a3a01546e52c1eb5a0b8fc418ef3abc", "first_paragraph": "Meta-learning[1][2] is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to", "has_infobox": false, "num_categories": 1, "num_images": 0, "num_infobox_rows": 0, "num_references": 28, "num_sections": 8, "page_length_chars": 18219, "scraped_at": "2026-01-07T00:30:04.535875", "title": "Meta-learning (computer science)", "url": "https://en.wikipedia.org/wiki/Meta-learning_(computer_science)"}
{"url_hash": "ba65b9feac84e6d2280b4aced54ada7c", "first_paragraph": "MLOps or ML Ops is a paradigm that aims to deploy and maintain machine learning models in production reliably and efficiently. It bridges the gap between machine learning development and production operations, ensuring that models are robust, scalable, and aligned with business goals. The word is a compound of \"machine learning\" and the continuous delivery practice (CI/CD) of DevOps in the software field. Machine learning models are tested and developed in isolated experimental systems. When an ", "has_infobox": false, "num_categories": 1, "num_images": 1, "num_infobox_rows": 0, "num_references": 15, "num_sections": 6, "page_length_chars": 7438, "scraped_at": "2026-01-07T00:30:07.092540", "title": "MLOps", "url": "https://en.wikipedia.org/wiki/MLOps"}
{"url_hash": "a5a0e85ebd4c05a15fece6fd3e79d686", "first_paragraph": "MobileNet is a family of convolutional neural network (CNN) architectures designed for image classification, object detection, and other computer vision tasks. They are designed for small size, low latency, and low power consumption, making them suitable for on-device inference and edge computing on resource-constrained devices like mobile phones and embedded systems. They were originally designed to be run efficiently on mobile devices with TensorFlow Lite.", "has_infobox": true, "num_categories": 3, "num_images": 9, "num_infobox_rows": 9, "num_references": 13, "num_sections": 8, "page_length_chars": 6568, "scraped_at": "2026-01-07T00:30:09.043457", "title": "MobileNet", "url": "https://en.wikipedia.org/wiki/MobileNet"}
{"url_hash": "9bd77fdd779350a68c9d0c56355ecd9e", "first_paragraph": "In machine learning, mode collapse is a failure mode observed in generative models, originally noted in Generative Adversarial Networks (GANs). It occurs when the model produces outputs that are less diverse than expected, effectively \"collapsing\" to generate only a few modes of the data distribution while ignoring others. This phenomenon undermines the goal of generative models to capture the full diversity of the training data.", "has_infobox": false, "num_categories": 3, "num_images": 0, "num_infobox_rows": 0, "num_references": 14, "num_sections": 5, "page_length_chars": 7995, "scraped_at": "2026-01-07T00:30:11.123006", "title": "Mode collapse", "url": "https://en.wikipedia.org/wiki/Mode_collapse"}
{"url_hash": "dc983b64bda398e5e65b63de431600ab", "first_paragraph": "Model compression is a machine learning technique for reducing the size of trained models. Large models can achieve high accuracy, but often at the cost of significant resource requirements. Compression techniques aim to compress models without significant performance reduction. Smaller models require less storage space, and consume less memory and compute during inference.[1]", "has_infobox": false, "num_categories": 2, "num_images": 10, "num_infobox_rows": 0, "num_references": 16, "num_sections": 6, "page_length_chars": 8957, "scraped_at": "2026-01-07T00:30:13.587427", "title": "Model compression", "url": "https://en.wikipedia.org/wiki/Model_compression"}
{"url_hash": "cc1da3e0d7597a58ccba0b1f93157576", "first_paragraph": "", "has_infobox": false, "num_categories": 1, "num_images": 12, "num_infobox_rows": 0, "num_references": 8, "num_sections": 17, "page_length_chars": 6533, "scraped_at": "2026-01-07T00:30:15.761895", "title": "Mountain car problem", "url": "https://en.wikipedia.org/wiki/Mountain_car_problem"}
{"url_hash": "0e622aef7a0d39d2b1e5d7d7bbaedd4e", "first_paragraph": "In probability theory and machine learning, the multi-armed bandit problem (sometimes called the K-[1] or N-armed bandit problem[2]) is named from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.[3]", "has_infobox": false, "num_categories": 6, "num_images": 101, "num_infobox_rows": 0, "num_references": 78, "num_sections": 23, "page_length_chars": 48357, "scraped_at": "2026-01-07T00:30:18.628843", "title": "Multi-armed bandit", "url": "https://en.wikipedia.org/wiki/Multi-armed_bandit"}
{"url_hash": "e554e04e5c862811b3517da1c24717f7", "first_paragraph": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Inherently, Multi-task learning is a multi-objective optimization problem having trade-offs between different tasks.[4] Early versions of MTL were called \"hi", "has_infobox": false, "num_categories": 1, "num_images": 92, "num_infobox_rows": 0, "num_references": 58, "num_sections": 15, "page_length_chars": 31882, "scraped_at": "2026-01-07T00:30:21.309776", "title": "Multi-task learning", "url": "https://en.wikipedia.org/wiki/Multi-task_learning"}
{"url_hash": "2967d23f3b121b1b3994a8ab21259b72", "first_paragraph": "Multimodal representation learning is a subfield of representation learning focused on integrating and interpreting information from different modalities, such as text, images, audio, or video, by projecting them into a shared latent space. This allows for semantically similar content across modalities to be mapped to nearby points within that space, facilitating a unified understanding of diverse data types.[1] By automatically learning meaningful features from each modality and capturing their", "has_infobox": false, "num_categories": 1, "num_images": 33, "num_infobox_rows": 0, "num_references": 14, "num_sections": 7, "page_length_chars": 11148, "scraped_at": "2026-01-07T00:30:23.877608", "title": "Multimodal representation learning", "url": "https://en.wikipedia.org/wiki/Multimodal_representation_learning"}
{"url_hash": "73eaf397c9899977f927bb020354844e", "first_paragraph": "Multimodal sentiment analysis is a technology for traditional text-based sentiment analysis, which includes modalities such as audio and visual data.[1] It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities.[2] With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment ", "has_infobox": false, "num_categories": 5, "num_images": 0, "num_infobox_rows": 0, "num_references": 24, "num_sections": 10, "page_length_chars": 12059, "scraped_at": "2026-01-07T00:30:25.706381", "title": "Multimodal sentiment analysis", "url": "https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis"}
{"url_hash": "972afaf9a1d459ccd36708fe2a4236d9", "first_paragraph": "In machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of label", "has_infobox": false, "num_categories": 1, "num_images": 106, "num_infobox_rows": 0, "num_references": 23, "num_sections": 16, "page_length_chars": 29394, "scraped_at": "2026-01-07T00:30:28.743568", "title": "Multiple instance learning", "url": "https://en.wikipedia.org/wiki/Multiple_instance_learning"}
{"url_hash": "e184dcc4c19c2b0eb02a5bce1bee03c6", "first_paragraph": "In machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of label", "has_infobox": false, "num_categories": 1, "num_images": 106, "num_infobox_rows": 0, "num_references": 23, "num_sections": 16, "page_length_chars": 29394, "scraped_at": "2026-01-07T00:30:31.471061", "title": "Multiple-instance learning", "url": "https://en.wikipedia.org/wiki/Multiple-instance_learning"}
{"url_hash": "04060e9ab5e50fa03df56371c8270e42", "first_paragraph": "The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively accord", "has_infobox": false, "num_categories": 3, "num_images": 123, "num_infobox_rows": 0, "num_references": 15, "num_sections": 14, "page_length_chars": 17347, "scraped_at": "2026-01-07T00:30:34.404186", "title": "Multiplicative weight update method", "url": "https://en.wikipedia.org/wiki/Multiplicative_weight_update_method"}
{"url_hash": "205d8059307b20c46d6088785942883f", "first_paragraph": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately.[1][2][3] Inherently, Multi-task learning is a multi-objective optimization problem having trade-offs between different tasks.[4] Early versions of MTL were called \"hi", "has_infobox": false, "num_categories": 1, "num_images": 92, "num_infobox_rows": 0, "num_references": 58, "num_sections": 15, "page_length_chars": 31882, "scraped_at": "2026-01-07T00:30:36.859490", "title": "Multitask optimization", "url": "https://en.wikipedia.org/wiki/Multitask_optimization"}
{"url_hash": "e060403d51a554042ac7fc80f6bda6e4", "first_paragraph": "In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991.[1] It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.", "has_infobox": false, "num_categories": 2, "num_images": 35, "num_infobox_rows": 0, "num_references": 9, "num_sections": 12, "page_length_chars": 14941, "scraped_at": "2026-01-07T00:30:39.224410", "title": "Multivariate adaptive regression spline", "url": "https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline"}
{"url_hash": "48f470804783795d39d871948aec041b", "first_paragraph": "Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2).[1] NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.", "has_infobox": false, "num_categories": 6, "num_images": 0, "num_infobox_rows": 0, "num_references": 10, "num_sections": 8, "page_length_chars": 5537, "scraped_at": "2026-01-07T00:30:41.626916", "title": "Native-language identification", "url": "https://en.wikipedia.org/wiki/Native-language_identification"}
{"url_hash": "ac029fc9a33048beed38e1fe7cf6b8e9", "first_paragraph": "Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema.[1]", "has_infobox": true, "num_categories": 8, "num_images": 5, "num_infobox_rows": 17, "num_references": 6, "num_sections": 4, "page_length_chars": 2702, "scraped_at": "2026-01-07T00:30:43.980905", "title": "Nature Machine Intelligence", "url": "https://en.wikipedia.org/wiki/Nature_Machine_Intelligence"}
{"url_hash": "4a5682a09533b33b304f85a432fbfc62", "first_paragraph": "Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS).[1][2][3][4] [5][6] This framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of the mind's mechanisms, including concepts, emotions, instincts, im", "has_infobox": false, "num_categories": 1, "num_images": 10, "num_infobox_rows": 0, "num_references": 10, "num_sections": 6, "page_length_chars": 20288, "scraped_at": "2026-01-07T00:30:45.706928", "title": "Neural modeling fields", "url": "https://en.wikipedia.org/wiki/Neural_modeling_fields"}
{"url_hash": "3bf96dc58510f4547336d0ea3c271f3d", "first_paragraph": "Neural Network Quantum States (NQS or NNQS) is a general class of variational quantum states parameterized in terms of an artificial neural network. It was first introduced in 2017 by the physicists Giuseppe Carleo and Matthias Troyer[1] to approximate wave functions of many-body quantum systems.", "has_infobox": false, "num_categories": 3, "num_images": 25, "num_infobox_rows": 0, "num_references": 3, "num_sections": 4, "page_length_chars": 3483, "scraped_at": "2026-01-07T00:30:47.361153", "title": "Neural network quantum states", "url": "https://en.wikipedia.org/wiki/Neural_network_quantum_states"}
{"url_hash": "d25914db41ead69839edbebb47bed5db", "first_paragraph": "In machine learning, normalization is a statistical technique with various applications. There are two main forms of normalization, namely data normalization and activation normalization. Data normalization (or feature scaling) includes methods that rescale input data so that the features have the same range, mean, variance, or other statistical properties. For instance, a popular choice of feature scaling method is min-max normalization, where each feature is transformed to have the same range ", "has_infobox": false, "num_categories": 4, "num_images": 130, "num_infobox_rows": 0, "num_references": 36, "num_sections": 19, "page_length_chars": 22704, "scraped_at": "2026-01-07T00:30:50.299583", "title": "Normalization (machine learning)", "url": "https://en.wikipedia.org/wiki/Normalization_(machine_learning)"}
{"url_hash": "50eaec37cbbb813a47ae3d4dcecffcf8", "first_paragraph": "Novelty detection is the mechanism by which an intelligent organism is able to identify an incoming sensory pattern as being hitherto unknown. If the pattern is sufficiently salient or associated with a high positive or strong negative utility, it will be given computational resources for effective future processing.", "has_infobox": false, "num_categories": 5, "num_images": 0, "num_infobox_rows": 0, "num_references": 6, "num_sections": 2, "page_length_chars": 2514, "scraped_at": "2026-01-07T00:30:52.420295", "title": "Novelty detection", "url": "https://en.wikipedia.org/wiki/Novelty_detection"}
{"url_hash": "19f717f1c9e27d5974d7578b09ac8877", "first_paragraph": "Offline learning is a machine learning training approach in which a model is trained on a fixed dataset that is not updated during the learning process.[1] This dataset is collected beforehand, and the learning typically occurs in a batch mode (i.e., the model is updated using batches of data, rather than a single input-output pair at a time). Once the model is trained, it can make predictions on new, unseen data.", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 2, "num_sections": 2, "page_length_chars": 1100, "scraped_at": "2026-01-07T00:30:54.634217", "title": "Offline learning", "url": "https://en.wikipedia.org/wiki/Offline_learning"}
{"url_hash": "59d1e3e9cfe8881363e47824ef11c4bc", "first_paragraph": "In mathematical modeling, overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably.[1] An overfitted model is a mathematical model that contains more parameters than can be justified by the data.[2] In the special case of a model that consists of a polynomial function, these parameters represent the degree of a polynomial. The essence of overfitting is", "has_infobox": false, "num_categories": 5, "num_images": 11, "num_infobox_rows": 0, "num_references": 17, "num_sections": 13, "page_length_chars": 21503, "scraped_at": "2026-01-07T00:30:57.099442", "title": "Overfitting", "url": "https://en.wikipedia.org/wiki/Overfitting"}
{"url_hash": "ebdcd1d11c69bc1625bb6b02bc19718d", "first_paragraph": "Paraphrase or paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases. Applications of paraphrasing are varied including information retrieval, question answering, text summarization, and plagiarism detection.[1] Paraphrasing is also useful in the evaluation of machine translation,[2] as well as semantic parsing[3] and generation[4] of new samples to expand existing corpora.[5]", "has_infobox": false, "num_categories": 2, "num_images": 30, "num_infobox_rows": 0, "num_references": 23, "num_sections": 13, "page_length_chars": 18897, "scraped_at": "2026-01-07T00:30:58.892582", "title": "Paraphrasing (computational linguistics)", "url": "https://en.wikipedia.org/wiki/Paraphrasing_(computational_linguistics)"}
{"url_hash": "63692e82df225134dad34bcf7ce3dd9f", "first_paragraph": "Parity learning is a problem in machine learning. An algorithm that solves this problem must find a function ƒ, given some samples (x, ƒ(x)) and the assurance that ƒ computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.", "has_infobox": false, "num_categories": 3, "num_images": 4, "num_infobox_rows": 0, "num_references": 2, "num_sections": 3, "page_length_chars": 2174, "scraped_at": "2026-01-07T00:31:01.202221", "title": "Parity learning", "url": "https://en.wikipedia.org/wiki/Parity_learning"}
{"url_hash": "44aee3876acbf7cc1db65360f9b7ed7a", "first_paragraph": "In theoretical computer science, a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables. Pattern Languages were introduced by Dana Angluin in the context of machine learning.[1]", "has_infobox": false, "num_categories": 3, "num_images": 1, "num_infobox_rows": 0, "num_references": 9, "num_sections": 6, "page_length_chars": 8982, "scraped_at": "2026-01-07T00:31:02.789970", "title": "Pattern language (formal languages)", "url": "https://en.wikipedia.org/wiki/Pattern_language_(formal_languages)"}
{"url_hash": "a507cb5f9432073674e1795cfa5dbd97", "first_paragraph": "Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess PR capabilities but their primary function is to distinguish and create emergent patterns. PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern re", "has_infobox": false, "num_categories": 4, "num_images": 48, "num_infobox_rows": 0, "num_references": 24, "num_sections": 19, "page_length_chars": 28164, "scraped_at": "2026-01-07T00:31:04.773382", "title": "Pattern recognition", "url": "https://en.wikipedia.org/wiki/Pattern_recognition"}
{"url_hash": "bea7e85cb5467fdcb4f0acb5eb0c396e", "first_paragraph": "Perceiver is a variant of the Transformer architecture, adapted for processing arbitrary forms of data, such as images, sounds and video, and spatial data. Unlike previous notable Transformer systems such as BERT and GPT-3, which were designed for text processing, the Perceiver is designed as a general architecture that can learn from large amounts of heterogeneous data. It accomplishes this with an asymmetric attention mechanism to distill inputs into a latent bottleneck.", "has_infobox": false, "num_categories": 1, "num_images": 0, "num_infobox_rows": 0, "num_references": 2, "num_sections": 7, "page_length_chars": 4026, "scraped_at": "2026-01-07T00:31:06.963972", "title": "Perceiver", "url": "https://en.wikipedia.org/wiki/Perceiver"}
{"url_hash": "5c2c9a085697a5f84924dcd107fed7e0", "first_paragraph": "", "has_infobox": true, "num_categories": 9, "num_images": 2, "num_infobox_rows": 8, "num_references": 10, "num_sections": 4, "page_length_chars": 9190, "scraped_at": "2026-01-07T00:31:09.385354", "title": "PHerc. Paris. 4", "url": "https://en.wikipedia.org/wiki/PHerc._Paris._4"}
{"url_hash": "3425bcd6c8704151ff1c6e484ac2878e", "first_paragraph": "In statistics, the phi coefficient, or mean square contingency coefficient, denoted by φ or rφ, is a measure of association for two binary variables.", "has_infobox": false, "num_categories": 8, "num_images": 43, "num_infobox_rows": 0, "num_references": 35, "num_sections": 9, "page_length_chars": 22589, "scraped_at": "2026-01-07T00:31:12.001855", "title": "Phi coefficient", "url": "https://en.wikipedia.org/wiki/Phi_coefficient"}
{"url_hash": "cfb4ff255c560083115596683b4cd65e", "first_paragraph": "Predictive learning is a machine learning (ML) technique where an artificial intelligence model is fed new data to develop an understanding of its environment, capabilities, and limitations. This technique finds application in many areas, including neuroscience, business, robotics, and computer vision. This concept was developed and expanded by French computer scientist Yann LeCun in 1988 during his career at Bell Labs, where he trained models to detect handwriting so that financial companies co", "has_infobox": false, "num_categories": 1, "num_images": 21, "num_infobox_rows": 0, "num_references": 6, "num_sections": 10, "page_length_chars": 6975, "scraped_at": "2026-01-07T00:31:14.398174", "title": "Predictive learning", "url": "https://en.wikipedia.org/wiki/Predictive_learning"}
{"url_hash": "99d2a89f82bd5227650f6371264cbcf8", "first_paragraph": "In computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system.[1] A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the syste", "has_infobox": false, "num_categories": 3, "num_images": 30, "num_infobox_rows": 0, "num_references": 3, "num_sections": 2, "page_length_chars": 4011, "scraped_at": "2026-01-07T00:31:16.944680", "title": "Predictive state representation", "url": "https://en.wikipedia.org/wiki/Predictive_state_representation"}
{"url_hash": "a31f84db9ab8b929a53575ce3413923a", "first_paragraph": "Preference learning is a subfield of machine learning that focuses on modeling and predicting preferences based on observed preference information.[1] Preference learning typically involves supervised learning using datasets of pairwise preference comparisons, rankings, or other preference information.", "has_infobox": false, "num_categories": 2, "num_images": 31, "num_infobox_rows": 0, "num_references": 7, "num_sections": 9, "page_length_chars": 5639, "scraped_at": "2026-01-07T00:31:19.550579", "title": "Preference learning", "url": "https://en.wikipedia.org/wiki/Preference_learning"}
{"url_hash": "dfce45c4020e7f42596abb925c08e0f5", "first_paragraph": "Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs\nthat form the training data (or training set). Nonetheless, in real world applications such as character recognition,", "has_infobox": false, "num_categories": 2, "num_images": 14, "num_infobox_rows": 0, "num_references": 1, "num_sections": 5, "page_length_chars": 3990, "scraped_at": "2026-01-07T00:31:22.119514", "title": "Prior knowledge for pattern recognition", "url": "https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition"}
{"url_hash": "ad322e4df4f3b3df2f7baf498f969778", "first_paragraph": "Proactive learning[1] is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.", "has_infobox": false, "num_categories": 3, "num_images": 1, "num_infobox_rows": 0, "num_references": 1, "num_sections": 1, "page_length_chars": 1596, "scraped_at": "2026-01-07T00:31:23.884192", "title": "Proactive learning", "url": "https://en.wikipedia.org/wiki/Proactive_learning"}
{"url_hash": "f2d41da14536e773889748f69bbfe7aa", "first_paragraph": "Proaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: Fuzzy Assignment Procedure for Nominal Sorting.", "has_infobox": false, "num_categories": 2, "num_images": 0, "num_infobox_rows": 0, "num_references": 8, "num_sections": 2, "page_length_chars": 3589, "scraped_at": "2026-01-07T00:31:25.591169", "title": "Proaftn", "url": "https://en.wikipedia.org/wiki/Proaftn"}
{"url_hash": "6c8151cc11331fc7a3ea6c9d061ee5b9", "first_paragraph": "Probabilistic numerics is an active field of study at the intersection of applied mathematics, statistics, and machine learning centering on the concept of uncertainty in computation. In probabilistic numerics, tasks in numerical analysis such as finding numerical solutions for integration, linear algebra, optimization and simulation and differential equations are seen as problems of statistical, probabilistic, or Bayesian inference.[1][2][3][4][5]", "has_infobox": false, "num_categories": 3, "num_images": 69, "num_infobox_rows": 0, "num_references": 55, "num_sections": 12, "page_length_chars": 27915, "scraped_at": "2026-01-07T00:31:27.834048", "title": "Probabilistic numerics", "url": "https://en.wikipedia.org/wiki/Probabilistic_numerics"}
{"url_hash": "2b8b7628d940044337723141454b9ea0", "first_paragraph": "Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of \"positive\" on 60% of instances, and a class label of \"negative\" on 40% of instances.", "has_infobox": false, "num_categories": 6, "num_images": 4, "num_infobox_rows": 0, "num_references": 0, "num_sections": 1, "page_length_chars": 2042, "scraped_at": "2026-01-07T00:31:29.835304", "title": "Probability matching", "url": "https://en.wikipedia.org/wiki/Probability_matching"}
{"url_hash": "e3b7f37d94e3371f3455f49bc27971b6", "first_paragraph": "Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions. It was proposed by Geoffrey Hinton in 1999,[1] along with an algorithm for training the parameters of such a system.", "has_infobox": false, "num_categories": 2, "num_images": 7, "num_infobox_rows": 0, "num_references": 2, "num_sections": 3, "page_length_chars": 1955, "scraped_at": "2026-01-07T00:31:32.341923", "title": "Product of experts", "url": "https://en.wikipedia.org/wiki/Product_of_experts"}
{"url_hash": "830f71b9aefbe055fc07c4bb0cea6dbb", "first_paragraph": "The American artificial intelligence (AI) organization OpenAI has released a variety of products and applications since its founding in 2013.", "has_infobox": false, "num_categories": 2, "num_images": 12, "num_infobox_rows": 0, "num_references": 165, "num_sections": 44, "page_length_chars": 56987, "scraped_at": "2026-01-07T00:31:34.447765", "title": "Products and applications of OpenAI", "url": "https://en.wikipedia.org/wiki/Products_and_applications_of_OpenAI"}
{"url_hash": "70293df96524b6b78c52906e9615630a", "first_paragraph": "In computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples.[1] The system records user actions and infers a generalized program that can be used on new examples.", "has_infobox": false, "num_categories": 5, "num_images": 3, "num_infobox_rows": 0, "num_references": 1, "num_sections": 3, "page_length_chars": 2522, "scraped_at": "2026-01-07T00:31:36.828213", "title": "Programming by example", "url": "https://en.wikipedia.org/wiki/Programming_by_example"}
{"url_hash": "dfa1482738e1a7500a713065eda52083", "first_paragraph": "Prompt engineering is the process of structuring or crafting an instruction in order to produce better outputs from a generative artificial intelligence (AI) model. It typically involves designing clear queries, adding relevant context, and refining wording to guide the model toward more accurate, useful, and consistent responses.[1]", "has_infobox": false, "num_categories": 7, "num_images": 27, "num_infobox_rows": 0, "num_references": 69, "num_sections": 19, "page_length_chars": 34037, "scraped_at": "2026-01-07T00:31:39.362649", "title": "Prompt engineering", "url": "https://en.wikipedia.org/wiki/Prompt_engineering"}
{"url_hash": "531832021f2f56f4eb128d77096d3687", "first_paragraph": "Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is\nregularization (also known as Lasso) of the form", "has_infobox": false, "num_categories": 3, "num_images": 96, "num_infobox_rows": 0, "num_references": 21, "num_sections": 13, "page_length_chars": 12959, "scraped_at": "2026-01-07T00:31:41.817891", "title": "Proximal gradient methods for learning", "url": "https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning"}
{"url_hash": "fcbbec2a7971466d0bd694321c4a87b5", "first_paragraph": "Purged cross-validation is a variant of k-fold cross-validation designed to prevent look-ahead bias in time series and other structured data, developed in 2017 by Marcos López de Prado at Guggenheim Partners and Cornell University.[1] It is primarily used in financial machine learning to ensure the independence of training and testing samples when labels depend on future events. It provides an alternative to conventional cross-validation and walk-forward backtesting methods, which often yield ov", "has_infobox": false, "num_categories": 2, "num_images": 11, "num_infobox_rows": 0, "num_references": 10, "num_sections": 13, "page_length_chars": 9943, "scraped_at": "2026-01-07T00:31:44.679492", "title": "Purged cross-validation", "url": "https://en.wikipedia.org/wiki/Purged_cross-validation"}
{"url_hash": "2175f268a2281ae4fc162e3351cb0f32", "first_paragraph": "Pythia[1][2] is an ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks. It was created by Yannis Assael, Thea Sommerschield, and Jonathan Prag, researchers from Google DeepMind and the University of Oxford.[3]", "has_infobox": false, "num_categories": 6, "num_images": 1, "num_infobox_rows": 0, "num_references": 4, "num_sections": 1, "page_length_chars": 2184, "scraped_at": "2026-01-07T00:31:47.162162", "title": "Pythia (machine learning)", "url": "https://en.wikipedia.org/wiki/Pythia_(machine_learning)"}
{"url_hash": "b730750aafa22d1cbfa07a3646e9dff7", "first_paragraph": "In machine learning and data mining, quantification (variously called learning to quantify, or supervised prevalence estimation, or class prior estimation) is the task of using supervised learning in order to train models (quantifiers) that estimate the relative frequencies (also known as prevalence values) of the classes of interest in a sample of unlabelled data items[1][2]. For instance, in a sample of 100,000 unlabelled tweets known to express opinions about a certain political candidate, a ", "has_infobox": false, "num_categories": 1, "num_images": 3, "num_infobox_rows": 0, "num_references": 35, "num_sections": 7, "page_length_chars": 13797, "scraped_at": "2026-01-07T00:31:48.953500", "title": "Quantification (machine learning)", "url": "https://en.wikipedia.org/wiki/Quantification_(machine_learning)"}
{"url_hash": "fb97bba2e382b0972008affd6b53e47f", "first_paragraph": "Quantum machine learning (QML) is the study of quantum algorithms for machine learning.[1][2][3][4] It often refers to quantum algorithms for machine learning tasks which analyze classical data, sometimes called quantum-enhanced machine learning.[5][6][7]", "has_infobox": false, "num_categories": 4, "num_images": 19, "num_infobox_rows": 0, "num_references": 118, "num_sections": 21, "page_length_chars": 65661, "scraped_at": "2026-01-07T00:31:51.302307", "title": "Quantum machine learning", "url": "https://en.wikipedia.org/wiki/Quantum_machine_learning"}
{"url_hash": "e8a663260e7d3169d9fed76dd3a3c020", "first_paragraph": "", "has_infobox": true, "num_categories": 5, "num_images": 3, "num_infobox_rows": 6, "num_references": 34, "num_sections": 11, "page_length_chars": 11963, "scraped_at": "2026-01-07T00:31:53.614772", "title": "Rabbit r1", "url": "https://en.wikipedia.org/wiki/Rabbit_r1"}
{"url_hash": "67664fffedea3c65661eefc6595a5e63", "first_paragraph": "In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of sets with respect to a probability distribution. The concept can also be extended to real valued functions.", "has_infobox": false, "num_categories": 2, "num_images": 161, "num_infobox_rows": 0, "num_references": 6, "num_sections": 16, "page_length_chars": 10067, "scraped_at": "2026-01-07T00:31:56.529268", "title": "Rademacher complexity", "url": "https://en.wikipedia.org/wiki/Rademacher_complexity"}
{"url_hash": "bde6118840fe31fba1cd49e8e574f9a3", "first_paragraph": "Random features (RF) are a technique used in machine learning to approximate kernel methods, introduced by Ali Rahimi and Ben Recht in their 2007 paper \"Random Features for Large-Scale Kernel Machines\",[1] and extended by.[2][3] RF uses a Monte Carlo approximation to kernel functions by randomly sampled feature maps. It is used for datasets that are too large for traditional kernel methods like support vector machine, kernel ridge regression, and gaussian process.", "has_infobox": false, "num_categories": 3, "num_images": 61, "num_infobox_rows": 0, "num_references": 7, "num_sections": 14, "page_length_chars": 6428, "scraped_at": "2026-01-07T00:31:59.428002", "title": "Random feature", "url": "https://en.wikipedia.org/wiki/Random_feature"}
{"url_hash": "7e6bdcb3177bac6c4d39182de0ffa6f3", "first_paragraph": "A reasoning model, also known as reasoning language models (RLMs) or large reasoning models (LRMs), is a type of large language model (LLM) that has been specifically trained to solve complex tasks requiring multiple steps of logical reasoning.[1] These models demonstrate superior performance on logic, mathematics, and programming tasks compared to standard LLMs. They possess the ability to revisit and revise earlier reasoning steps and utilize additional computation during inference as a method", "has_infobox": true, "num_categories": 5, "num_images": 24, "num_infobox_rows": 6, "num_references": 64, "num_sections": 12, "page_length_chars": 33235, "scraped_at": "2026-01-07T00:32:01.671106", "title": "Reasoning model", "url": "https://en.wikipedia.org/wiki/Reasoning_model"}
{"url_hash": "ace874a4ac91053d620b386210f47232", "first_paragraph": "Reciprocal Human Machine Learning (RHML) is an interdisciplinary approach to designing human-AI interaction systems.[1] RHML aims to enable continual learning between humans and machine learning models by having them learn from each other. This approach keeps the human expert \"in the loop\" to oversee and enhance machine learning performance and simultaneously support the human expert continue learning.", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 10, "num_sections": 3, "page_length_chars": 5354, "scraped_at": "2026-01-07T00:32:04.239410", "title": "Reciprocal human machine learning", "url": "https://en.wikipedia.org/wiki/Reciprocal_human_machine_learning"}
{"url_hash": "d38e47d9b0e7d0acf7f89ec65c35373c", "first_paragraph": "Relational data mining is the data mining technique for relational databases.[1] Unlike traditional data mining algorithms, which look for patterns in a single table (propositional patterns), relational data mining algorithms look for patterns among multiple tables (relational patterns). For most types of propositional patterns, there are corresponding relational patterns. For example, there are relational classification rules (relational classification), relational regression tree, and relation", "has_infobox": false, "num_categories": 5, "num_images": 2, "num_infobox_rows": 0, "num_references": 2, "num_sections": 6, "page_length_chars": 2360, "scraped_at": "2026-01-07T00:32:06.163353", "title": "Relational data mining", "url": "https://en.wikipedia.org/wiki/Relational_data_mining"}
{"url_hash": "f775d38ac282776159559a3ddc511230", "first_paragraph": "The reparameterization trick (aka \"reparameterization gradient estimator\") is a technique used in statistical machine learning, particularly in variational inference, variational autoencoders, and stochastic optimization. It allows for the efficient computation of gradients through random variables, enabling the optimization of parametric probability models using stochastic gradient descent, and the variance reduction of estimators.", "has_infobox": false, "num_categories": 2, "num_images": 55, "num_infobox_rows": 0, "num_references": 7, "num_sections": 11, "page_length_chars": 6085, "scraped_at": "2026-01-07T00:32:08.264413", "title": "Reparameterization trick", "url": "https://en.wikipedia.org/wiki/Reparameterization_trick"}
{"url_hash": "0152abb15acb6fb6b9643ffca8abf637", "first_paragraph": "In the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to [an] explanation is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports", "has_infobox": false, "num_categories": 5, "num_images": 1, "num_infobox_rows": 0, "num_references": 22, "num_sections": 8, "page_length_chars": 17656, "scraped_at": "2026-01-07T00:32:10.082856", "title": "Right to explanation", "url": "https://en.wikipedia.org/wiki/Right_to_explanation"}
{"url_hash": "af1bdabd499a5d686065e49d6ca6aeff", "first_paragraph": "Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives", "has_infobox": false, "num_categories": 3, "num_images": 3, "num_infobox_rows": 0, "num_references": 11, "num_sections": 6, "page_length_chars": 7735, "scraped_at": "2026-01-07T00:32:11.777481", "title": "Robot learning", "url": "https://en.wikipedia.org/wiki/Robot_learning"}
{"url_hash": "6d49cf593ed2f4c4a7c2e383e1b467d6", "first_paragraph": "Robotic process automation (RPA) is a form of business process automation that is based on software robots (bots) or artificial intelligence (AI) agents.[1] RPA should not be confused with artificial intelligence as it is based on automation technology following a predefined workflow.[2] It is sometimes referred to as software robotics (not to be confused with robot software).", "has_infobox": false, "num_categories": 5, "num_images": 2, "num_infobox_rows": 0, "num_references": 27, "num_sections": 15, "page_length_chars": 23237, "scraped_at": "2026-01-07T00:32:13.764882", "title": "Robotic process automation", "url": "https://en.wikipedia.org/wiki/Robotic_process_automation"}
{"url_hash": "eaa18928d1ae97ad04a3d2bf72ca82fb", "first_paragraph": "", "has_infobox": true, "num_categories": 11, "num_images": 17, "num_infobox_rows": 18, "num_references": 47, "num_sections": 23, "page_length_chars": 17832, "scraped_at": "2026-01-07T00:32:16.211223", "title": "ROCm", "url": "https://en.wikipedia.org/wiki/ROCm"}
{"url_hash": "8a84b80b0bc8ea949120112e7486a47d", "first_paragraph": "Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.", "has_infobox": false, "num_categories": 3, "num_images": 2, "num_infobox_rows": 0, "num_references": 4, "num_sections": 3, "page_length_chars": 2434, "scraped_at": "2026-01-07T00:32:18.163266", "title": "Rule induction", "url": "https://en.wikipedia.org/wiki/Rule_induction"}
{"url_hash": "9e3965cb1c9e441aa428bd4a5bbad4ae", "first_paragraph": "The sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.", "has_infobox": false, "num_categories": 1, "num_images": 103, "num_infobox_rows": 0, "num_references": 13, "num_sections": 9, "page_length_chars": 9408, "scraped_at": "2026-01-07T00:32:20.786376", "title": "Sample complexity", "url": "https://en.wikipedia.org/wiki/Sample_complexity"}
{"url_hash": "367335bb2f2c8c4776bf3cf382517eef", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 8, "num_infobox_rows": 0, "num_references": 21, "num_sections": 10, "page_length_chars": 16491, "scraped_at": "2026-01-07T00:32:22.507666", "title": "Self-supervised learning", "url": "https://en.wikipedia.org/wiki/Self-supervised_learning"}
{"url_hash": "07d2d9e35cf8b36c4a66f76296ffaadf", "first_paragraph": "In machine learning, semantic analysis of a text corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents.", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 2, "num_sections": 2, "page_length_chars": 4664, "scraped_at": "2026-01-07T00:32:24.088549", "title": "Semantic analysis (machine learning)", "url": "https://en.wikipedia.org/wiki/Semantic_analysis_(machine_learning)"}
{"url_hash": "0188737d4d4e47cde88821bacdf21a37", "first_paragraph": "Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.[1]", "has_infobox": false, "num_categories": 4, "num_images": 2, "num_infobox_rows": 0, "num_references": 21, "num_sections": 6, "page_length_chars": 11168, "scraped_at": "2026-01-07T00:32:26.249045", "title": "Semantic folding", "url": "https://en.wikipedia.org/wiki/Semantic_folding"}
{"url_hash": "6504615e972451663dc33142bb427ca9", "first_paragraph": "Weak supervision (also known as semi-supervised learning) is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to the large amount of data required to train them. It is characterized by using a combination of a small amount of human-labeled data (exclusively used in more expensive and time-consuming supervised learning paradigm), followed by a large amount of unlabeled data (used exclusively in unsupervised learning parad", "has_infobox": false, "num_categories": 1, "num_images": 48, "num_infobox_rows": 0, "num_references": 22, "num_sections": 18, "page_length_chars": 19263, "scraped_at": "2026-01-07T00:32:28.241147", "title": "Semi-supervised learning", "url": "https://en.wikipedia.org/wiki/Semi-supervised_learning"}
{"url_hash": "6a06481cddf5bfae522a3df251c4c964", "first_paragraph": "In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved b", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 0, "num_sections": 3, "page_length_chars": 3489, "scraped_at": "2026-01-07T00:32:30.598922", "title": "Sequence labeling", "url": "https://en.wikipedia.org/wiki/Sequence_labeling"}
{"url_hash": "d8d1aec38d071f232eb5e89c956a873c", "first_paragraph": "Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.", "has_infobox": false, "num_categories": 2, "num_images": 34, "num_infobox_rows": 0, "num_references": 16, "num_sections": 8, "page_length_chars": 7682, "scraped_at": "2026-01-07T00:32:33.147690", "title": "Similarity learning", "url": "https://en.wikipedia.org/wiki/Similarity_learning"}
{"url_hash": "ba7a1faa715e59fba91959c9138fc1e3", "first_paragraph": "A socially assistive robot (SAR) aids users through social engagement and support rather than through physical tasks and interactions.", "has_infobox": false, "num_categories": 3, "num_images": 0, "num_infobox_rows": 0, "num_references": 12, "num_sections": 4, "page_length_chars": 6263, "scraped_at": "2026-01-07T00:32:35.555185", "title": "Socially assistive robot", "url": "https://en.wikipedia.org/wiki/Socially_assistive_robot"}
{"url_hash": "aebe36797fb5ba3310c7b2bc311a9229", "first_paragraph": "Lynda Soderholm is a physical chemist at the U.S. Department of Energy's (DOE) Argonne National Laboratory with a specialty in f-block elements.[1] She is a senior scientist and the lead of the Actinide, Geochemistry & Separation Sciences Theme within Argonne's Chemical Sciences and Engineering Division. Her specific role is the Separation Science group leader within Heavy Element Chemistry and Separation Science (HESS), directing basic research focused on low-energy methods for isolating lantha", "has_infobox": true, "num_categories": 11, "num_images": 2, "num_infobox_rows": 7, "num_references": 7, "num_sections": 10, "page_length_chars": 8008, "scraped_at": "2026-01-07T00:32:37.438986", "title": "Lynda Soderholm", "url": "https://en.wikipedia.org/wiki/Lynda_Soderholm"}
{"url_hash": "1cd3da323233d744ff66749810795fe7", "first_paragraph": "Solomonoff's theory of inductive inference proves that, under its common sense assumptions (axioms), the best possible scientific model is the shortest algorithm that generates the empirical data under consideration. In addition to the choice of data, other assumptions are that, to avoid the post-hoc fallacy, the programming language must be chosen prior to the data[1] and that the environment being observed is generated by an unknown algorithm. This is also called a theory of induction. Due to ", "has_infobox": false, "num_categories": 6, "num_images": 23, "num_infobox_rows": 0, "num_references": 15, "num_sections": 13, "page_length_chars": 13557, "scraped_at": "2026-01-07T00:32:39.825529", "title": "Solomonoff's theory of inductive inference", "url": "https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference"}
{"url_hash": "26c80d7d5f9c85dc900fcb9668b93cbb", "first_paragraph": "Spatial embedding is one of feature learning techniques used in spatial analysis where points, lines, polygons or other spatial data types.[1] representing geographic locations are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per geographic object to a continuous vector space with a much lower dimension.", "has_infobox": false, "num_categories": 3, "num_images": 7, "num_infobox_rows": 0, "num_references": 24, "num_sections": 10, "page_length_chars": 14177, "scraped_at": "2026-01-07T00:32:42.127239", "title": "Spatial embedding", "url": "https://en.wikipedia.org/wiki/Spatial_embedding"}
{"url_hash": "288e67a6f1c7a73da28c1d8394b6bc2f", "first_paragraph": "Spike-and-slab regression is a type of Bayesian linear regression in which a particular hierarchical prior distribution for the regression coefficients is chosen such that only a subset of the possible regressors is retained. The technique is particularly useful when the number of possible predictors is larger than the number of observations.[1] The idea of the spike-and-slab model was originally proposed by Mitchell & Beauchamp (1988).[2] The approach was further significantly developed by Madi", "has_infobox": false, "num_categories": 3, "num_images": 2, "num_infobox_rows": 0, "num_references": 8, "num_sections": 4, "page_length_chars": 4867, "scraped_at": "2026-01-07T00:32:43.857303", "title": "Spike-and-slab regression", "url": "https://en.wikipedia.org/wiki/Spike-and-slab_regression"}
{"url_hash": "93122985d5c822878451a4668f031cb2", "first_paragraph": "Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm output is changed with small perturbations to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (\"A", "has_infobox": false, "num_categories": 2, "num_images": 66, "num_infobox_rows": 0, "num_references": 11, "num_sections": 14, "page_length_chars": 13199, "scraped_at": "2026-01-07T00:32:46.344482", "title": "Stability (learning theory)", "url": "https://en.wikipedia.org/wiki/Stability_(learning_theory)"}
{"url_hash": "118d062c73567ce7bbdf1b29548bbd5b", "first_paragraph": "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1][2][3] Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.", "has_infobox": false, "num_categories": 2, "num_images": 44, "num_infobox_rows": 0, "num_references": 8, "num_sections": 9, "page_length_chars": 9101, "scraped_at": "2026-01-07T00:32:48.410793", "title": "Statistical learning theory", "url": "https://en.wikipedia.org/wiki/Statistical_learning_theory"}
{"url_hash": "482ca33eb430438e02ad6848442c8a15", "first_paragraph": "Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.[1][2] Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical ", "has_infobox": false, "num_categories": 2, "num_images": 1, "num_infobox_rows": 0, "num_references": 5, "num_sections": 5, "page_length_chars": 5372, "scraped_at": "2026-01-07T00:32:50.759264", "title": "Statistical relational learning", "url": "https://en.wikipedia.org/wiki/Statistical_relational_learning"}
{"url_hash": "4872972ce9d0a6727625200e323866a3", "first_paragraph": "Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data. This principle was first se", "has_infobox": false, "num_categories": 2, "num_images": 18, "num_infobox_rows": 0, "num_references": 2, "num_sections": 3, "page_length_chars": 2537, "scraped_at": "2026-01-07T00:32:53.062192", "title": "Structural risk minimization", "url": "https://en.wikipedia.org/wiki/Structural_risk_minimization"}
{"url_hash": "808305d0f863a5edad9b9abe8275abcb", "first_paragraph": "Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods.[1] Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable\n(i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space\n(i.e., the domain, space of features or explanatory variables). Sparsity r", "has_infobox": false, "num_categories": 3, "num_images": 133, "num_infobox_rows": 0, "num_references": 16, "num_sections": 17, "page_length_chars": 18221, "scraped_at": "2026-01-07T00:32:55.272103", "title": "Structured sparsity regularization", "url": "https://en.wikipedia.org/wiki/Structured_sparsity_regularization"}
{"url_hash": "5a9e4533971840afcfbebc63c8248c74", "first_paragraph": "A surrogate model is an engineering method used when an outcome of interest cannot be easily measured or computed, so an approximate mathematical model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as a function of design variables. For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the airflow around the wing for different shape variabl", "has_infobox": false, "num_categories": 5, "num_images": 1, "num_infobox_rows": 0, "num_references": 13, "num_sections": 10, "page_length_chars": 13421, "scraped_at": "2026-01-07T00:32:57.054008", "title": "Surrogate model", "url": "https://en.wikipedia.org/wiki/Surrogate_model"}
{"url_hash": "ff2ed8afb74ffd0e82d0351625a7fe55", "first_paragraph": "", "has_infobox": false, "num_categories": 4, "num_images": 4, "num_infobox_rows": 0, "num_references": 12, "num_sections": 9, "page_length_chars": 11616, "scraped_at": "2026-01-07T00:32:58.873620", "title": "Symbolic regression", "url": "https://en.wikipedia.org/wiki/Symbolic_regression"}
{"url_hash": "c264e84bfb8321cc9ece50f317d0f6ad", "first_paragraph": "In machine learning, the term tensor informally refers to two different concepts (i) a way of organizing data and (ii) a multilinear (tensor) transformation. Data may be organized in a multidimensional array (M-way array), informally referred to as a \"data tensor\"; however, in the strict mathematical sense, a tensor is a multilinear mapping over a set of domain vector spaces to a range vector space. Observations, such as images, movies, volumes, sounds, and relationships among words and concepts", "has_infobox": false, "num_categories": 3, "num_images": 75, "num_infobox_rows": 0, "num_references": 43, "num_sections": 9, "page_length_chars": 22133, "scraped_at": "2026-01-07T00:33:01.058493", "title": "Tensor (machine learning)", "url": "https://en.wikipedia.org/wiki/Tensor_(machine_learning)"}
{"url_hash": "ab253c9c5fc77ae1d4782b752cd4e672", "first_paragraph": "TensorFlow Hub (also styled TF Hub) is an open-source machine learning library and online repository that provides reusable TensorFlow model components, called modules.[1]", "has_infobox": true, "num_categories": 6, "num_images": 2, "num_infobox_rows": 14, "num_references": 9, "num_sections": 6, "page_length_chars": 4137, "scraped_at": "2026-01-07T00:33:03.152620", "title": "TensorFlow Hub", "url": "https://en.wikipedia.org/wiki/TensorFlow_Hub"}
{"url_hash": "b39b4a98fcc54ea850c6bee90d0ee443", "first_paragraph": "In neuroscience and machine learning, three-factor learning is the combination of Hebbian plasticity with a third modulatory factor to stabilise and enhance synaptic learning.[1] This third factor can represent various signals such as reward, punishment, error, surprise, or novelty, often implemented through neuromodulators.[2]", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 4, "num_sections": 2, "page_length_chars": 2343, "scraped_at": "2026-01-07T00:33:05.420856", "title": "Three-factor learning", "url": "https://en.wikipedia.org/wiki/Three-factor_learning"}
{"url_hash": "22f08af55c3f78b806aba1078bca7b65", "first_paragraph": "", "has_infobox": false, "num_categories": 5, "num_images": 11, "num_infobox_rows": 0, "num_references": 64, "num_sections": 29, "page_length_chars": 41054, "scraped_at": "2026-01-07T00:33:07.820670", "title": "Time series", "url": "https://en.wikipedia.org/wiki/Time_series"}
{"url_hash": "0904586bab3a70791c2bec98552c9386", "first_paragraph": "", "has_infobox": false, "num_categories": 2, "num_images": 2, "num_infobox_rows": 0, "num_references": 67, "num_sections": 6, "page_length_chars": 28141, "scraped_at": "2026-01-07T00:33:09.680543", "title": "Timeline of machine learning", "url": "https://en.wikipedia.org/wiki/Timeline_of_machine_learning"}
{"url_hash": "cb2f988fbaa9e6dde1164dc09f953c9a", "first_paragraph": "The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation.[1] It was published at RightsCon on May 16, 2018.[2][3]", "has_infobox": false, "num_categories": 6, "num_images": 0, "num_infobox_rows": 0, "num_references": 7, "num_sections": 7, "page_length_chars": 6472, "scraped_at": "2026-01-07T00:33:11.962141", "title": "Toronto Declaration", "url": "https://en.wikipedia.org/wiki/Toronto_Declaration"}
{"url_hash": "dfe5f0a0d7d89e905e2e492b2c09064e", "first_paragraph": "In logic, statistical inference, and supervised learning, transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model. Note that this is caused by transductive inference on differen", "has_infobox": false, "num_categories": 1, "num_images": 2, "num_infobox_rows": 0, "num_references": 2, "num_sections": 10, "page_length_chars": 10024, "scraped_at": "2026-01-07T00:33:14.050452", "title": "Transduction (machine learning)", "url": "https://en.wikipedia.org/wiki/Transduction_(machine_learning)"}
{"url_hash": "3fcc29c127bd186329e75098901899d9", "first_paragraph": "Transfer learning (TL) is a technique in machine learning (ML) in which knowledge learned from a task is re-used in order to boost performance on a related task.[1] For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. This topic is related to the psychological literature on transfer of learning, although practical ties between the two fields are limited. Reusing or transferring information from previously learn", "has_infobox": false, "num_categories": 1, "num_images": 26, "num_infobox_rows": 0, "num_references": 29, "num_sections": 6, "page_length_chars": 10769, "scraped_at": "2026-01-07T00:33:16.897789", "title": "Transfer learning", "url": "https://en.wikipedia.org/wiki/Transfer_learning"}
{"url_hash": "e47ac4b0d5bae484ff02d69544c5c0a9", "first_paragraph": "The ugly duckling theorem is an argument showing that classification is not really possible without some sort of bias. More particularly, it assumes finitely many properties combinable by logical connectives, and finitely many objects; it asserts that any two different objects share the same number of (extensional) properties. The theorem is named after Hans Christian Andersen's 1843 story \"The Ugly Duckling\", because it shows that a duckling is just as similar to a swan as two swans are to each", "has_infobox": false, "num_categories": 7, "num_images": 28, "num_infobox_rows": 0, "num_references": 11, "num_sections": 5, "page_length_chars": 10076, "scraped_at": "2026-01-07T00:33:19.306012", "title": "Ugly duckling theorem", "url": "https://en.wikipedia.org/wiki/Ugly_duckling_theorem"}
{"url_hash": "b5cfe2021b318011dcdb531a5eefc4de", "first_paragraph": "In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertain", "has_infobox": false, "num_categories": 3, "num_images": 0, "num_infobox_rows": 0, "num_references": 2, "num_sections": 2, "page_length_chars": 3979, "scraped_at": "2026-01-07T00:33:21.417808", "title": "Uncertain data", "url": "https://en.wikipedia.org/wiki/Uncertain_data"}
