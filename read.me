# Wikipedia ML Insights

> End-to-end data pipeline for clustering Wikipedia Machine Learning articles using Selenium, MongoDB, and Spark MLlib

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5.0-orange.svg)](https://spark.apache.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-7.0-green.svg)](https://www.mongodb.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## Overview

This project implements a production-ready data pipeline that:
- Collects 250 Wikipedia articles from the Machine Learning category
- Extracts 12 structural and content features per article
- Stores data in MongoDB with automatic deduplication
- Performs K-Means clustering using Apache Spark MLlib
- Discovers meaningful patterns in Wikipedia content quality

**Key Achievement:** Identified 2 distinct article types with 0.84 silhouette score - "Regular Technical Articles" vs "Featured Encyclopedia Articles"

---

## Features

### Data Collection
- **Automated scraping** with Selenium WebDriver
- **Pagination handling** across Wikipedia category pages
- **Robust extraction** with 0% missing data
- **Polite crawling** with configurable delays

### Data Storage
- **MongoDB integration** for flexible schema
- **Automatic deduplication** via unique indexes
- **Idempotent pipeline** supporting re-runs
- **JSONL export** for Spark compatibility

### Machine Learning
- **K-Means clustering** (k=2 to k=8 evaluation)
- **StandardScaler** for multi-scale features
- **Silhouette analysis** for optimal k selection
- **Cluster profiling** with statistical analysis

### Experimental Validation
- Wait strategy optimization (31% speedup)
- Headless vs non-headless comparison
- Rate limiting impact analysis
- MongoDB upsert validation
- Spark parallelism tuning
- Feature scaling impact assessment

---

## Technology Stack

**Web Scraping**
- Selenium 4.15.2
- Chrome WebDriver
- webdriver-manager

**Data Storage**
- MongoDB 7.0 (Docker)
- pymongo 4.6.1

**Data Processing**
- Apache Spark 3.5.0
- MLlib (K-Means, StandardScaler)
- pandas, numpy

**Infrastructure**
- Docker & Docker Compose
- Python 3.12

---

## Quick Start

### Prerequisites

```bash
# Required
Python 3.12+
Docker Desktop
Chrome Browser
Java 11+ (for Spark)

# Recommended
8GB RAM
4+ CPU cores
5GB disk space
```

### Installation

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/wikipedia-ml-insights.git
cd wikipedia-ml-insights

# Install Python dependencies
pip install -r requirements.txt

# Start MongoDB
docker-compose up -d

# Verify setup
docker ps | grep mongodb
```

### Running the Pipeline

**Option A: Full Pipeline** (~40 minutes)
```bash
# 1. Scrape articles (30 min)
python wikipedia_scraper.py

# 2. Run clustering (5 min)
python spark_clustering.py

# 3. Run experiments (optional, 20 min)
python experiments.py
```

**Option B: Use Existing Data** (5 minutes)
```bash
# Skip scraping, use provided data
python spark_clustering.py
```

---

## Project Structure

```
wikipedia-ml-insights/
├── README.md                      # This file
├── REPORT.md                      # Detailed documentation
├── requirements.txt               # Python dependencies
├── docker-compose.yml             # MongoDB setup
│
├── Core Pipeline
│   ├── wikipedia_scraper.py       # Selenium scraper
│   ├── spark_clustering.py        # K-Means clustering
│   └── experiments.py             # 6 experiments
│
├── Data Files
│   ├── wikipedia_urls.json        # 250 URLs collected
│   ├── wikipedia_features.jsonl   # Feature dataset
│   ├── clustering_results.csv     # Cluster assignments
│   ├── cluster_profiles.csv       # Cluster statistics
│   ├── kmeans_evaluation.csv      # K selection analysis
│   └── experiment_*.json          # Experiment results (6 files)
│
└── Documentation
    └── REPORT.md                  # Technical report
```

---

## Results Summary

### Clustering Performance

| Metric | Value |
|--------|-------|
| Dataset Size | 250 articles |
| Features Used | 6 numeric |
| Optimal k | 2 |
| Silhouette Score | 0.8410 (excellent) |
| Execution Time | ~40 minutes |

### Discovered Clusters

**Cluster 0 (n=241): Regular Technical Articles**
- Average length: 13,520 characters
- Average sections: 9.1
- Average references: 18.5
- Typical examples: "Deep learning", "Neural network", "Support vector machine"

**Cluster 1 (n=9): Featured Encyclopedia Articles**
- Average length: 89,234 characters (6.6x larger)
- Average sections: 35.7 (3.9x more)
- Average references: 256.8 (13.9x more)
- Typical examples: "Artificial intelligence", "Machine learning", "Computer vision"

### Key Findings

1. **StandardScaler Critical** - Without scaling, clustering becomes trivial (size-based only). With scaling, silhouette improves by 159%.

2. **Wikipedia is Stable** - Strict waits unnecessary (saves 31% time). Aggressive rate limiting safe for <500 pages.

3. **MongoDB Upsert Essential** - Guarantees idempotency. Without it, re-runs create duplicates.

4. **Spark Optimal at Hardware Level** - 8 partitions for 8 cores provides ~2x speedup. Beyond that, diminishing returns.

---

## Experiments

Six controlled experiments validated design decisions:

| Experiment | Key Finding |
|-----------|-------------|
| 1. Wait Strategies | Strict waits +31% slower, 0% quality gain for Wikipedia |
| 2. Headless Mode | 10% faster than non-headless, same reliability |
| 3. Rate Limiting | Fast policy 2.5x faster, zero penalties from Wikipedia |
| 4. MongoDB Upsert | Prevents duplicates, negligible overhead |
| 5. Spark Parallelism | Optimal at core count (8 partitions = 8 cores) |
| 6. StandardScaler | +159% silhouette improvement, enables meaningful patterns |

See `REPORT.md` for detailed experimental methodology.

---

## Usage Examples

### Basic Clustering

```python
from spark_clustering import main_clustering_pipeline

# Run clustering with default settings
results_df, profiles = main_clustering_pipeline(
    jsonl_file="wikipedia_features.jsonl",
    use_scaling=True,
    k_range=range(2, 9)
)
```

### Custom Scraping

```python
from wikipedia_scraper import scrape_articles

# Scrape with custom parameters
scrape_articles(
    seed_url="https://en.wikipedia.org/wiki/Category:Machine_learning",
    target_count=250,
    headless=True,
    delay_range=(1.0, 2.0)
)
```

---

## Architecture

```
Wikipedia Category → Selenium Scraper → MongoDB → JSONL Export
                                          ↓
                                   Spark DataFrame
                                          ↓
                              VectorAssembler (6 features)
                                          ↓
                                  StandardScaler
                                          ↓
                                K-Means Clustering
                                          ↓
                        Cluster Analysis & Profiling
```

---

## Performance

### Scraping Performance
- **Speed:** ~7 seconds per article
- **Success Rate:** 100% (0% failures)
- **Data Quality:** 0% missing values
- **Total Time:** ~30 minutes for 250 articles

### Clustering Performance
- **Loading:** ~10 seconds (JSONL to DataFrame)
- **Feature Assembly:** ~5 seconds
- **Scaling:** ~3 seconds
- **K-Means (k=2-8):** ~5 minutes total
- **Total Time:** ~5-6 minutes

---

## Requirements

### Python Dependencies
```
selenium==4.15.2
webdriver-manager==4.0.1
pymongo==4.6.1
pyspark==3.5.0
pandas==2.1.4
numpy==1.26.2
```

### System Requirements
- Python 3.12+
- Docker Desktop
- Chrome Browser
- Java 11+ (for Spark)
- 8GB RAM (minimum)
- Internet connection (for scraping)

---

## Troubleshooting

### MongoDB Connection Issues
```bash
# Check if MongoDB is running
docker ps | grep mongodb

# Restart if needed
docker-compose down
docker-compose up -d
```

### Selenium WebDriver Issues
```bash
# Update webdriver-manager
pip install --upgrade webdriver-manager

# Manually download ChromeDriver if needed
# It should auto-download the correct version
```

### Spark Memory Issues
```bash
# Increase Spark driver memory
export SPARK_DRIVER_MEMORY=4g

# Or modify spark_clustering.py:
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
```

---

## Contributing

This is an academic project and is not actively maintained. However, feel free to fork and adapt for your own use cases.

### Potential Extensions
- Expand to other Wikipedia categories
- Add NLP features (TF-IDF, embeddings)
- Try other clustering algorithms (DBSCAN, Hierarchical)
- Cross-language analysis
- Temporal analysis (track article evolution)

---

## Citation

If you use this work, please cite:
```
Wikipedia ML Insights: End-to-end clustering pipeline
Year: 2026
URL: https://github.com/YOUR_USERNAME/wikipedia-ml-insights
```

---

## License

MIT License - See LICENSE file for details

---

## Acknowledgments

- Wikipedia for providing public access to article data
- Apache Spark community for MLlib
- Selenium project for web automation tools

---

## Contact

For questions or suggestions, please open an issue on GitHub.

**Note:** This is an academic project demonstrating distributed data processing and unsupervised learning techniques.